#!/usr/bin/env python3
"""
æ™ºèƒ½å¢žé‡åŒæ­¥å·¥å…· - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
é›†æˆè¡¨ç»“æž„ç¼“å­˜ã€è¿žæŽ¥æ± å¤ç”¨ã€æ‰¹é‡æ•°æ®å¤„ç†ç­‰æ€§èƒ½ä¼˜åŒ–
"""

import mysql.connector
import mysql.connector.pooling
from google.cloud import bigquery
import json
import sys
from datetime import datetime, timedelta
from decimal import Decimal
import time
import logging
from typing import Dict, List, Optional, Tuple
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sync_incremental.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# MySQL -> BigQuery ç±»åž‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

# å¸¸è§æ—¶é—´æˆ³å­—æ®µåï¼ˆæŒ‰ä¼˜å…ˆçº§æŽ’åºï¼‰
TIMESTAMP_FIELDS = [
    'updated_at', 'update_time', 'last_updated', 'modified_at', 'last_modified',
    'created_at', 'create_time', 'insert_time', 'timestamp', 'sync_time'
]

class TableInfoCache:
    """è¡¨ä¿¡æ¯ç¼“å­˜ç±»"""
    
    def __init__(self):
        self._cache = {}
        self._lock = threading.Lock()
    
    def get_table_info(self, db_name: str, table_name: str) -> Optional[Dict]:
        """èŽ·å–ç¼“å­˜çš„è¡¨ä¿¡æ¯"""
        key = f"{db_name}.{table_name}"
        with self._lock:
            return self._cache.get(key)
    
    def set_table_info(self, db_name: str, table_name: str, info: Dict):
        """è®¾ç½®è¡¨ä¿¡æ¯ç¼“å­˜"""
        key = f"{db_name}.{table_name}"
        with self._lock:
            self._cache[key] = info
            logger.info(f"  ðŸ’¾ ç¼“å­˜è¡¨ä¿¡æ¯: {key}")
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()

class LocalFileStatusManager:
    """æœ¬åœ°æ–‡ä»¶çŠ¶æ€ç®¡ç†å™¨ - æŒ‰æ•°æ®åº“åˆ†ç»„"""
    
    def __init__(self, status_dir: str = "sync_status"):
        self.status_dir = Path(status_dir)
        self.status_dir.mkdir(exist_ok=True)
        self._lock = threading.Lock()
        logger.info(f"âœ… æœ¬åœ°çŠ¶æ€ç›®å½•å·²å‡†å¤‡å°±ç»ª: {self.status_dir}")
    
    def _get_status_file(self, tenant_id: str) -> Path:
        """èŽ·å–æ•°æ®åº“çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
        return self.status_dir / f"{tenant_id}.json"
    
    def _load_database_status(self, tenant_id: str) -> Dict:
        """åŠ è½½æ•°æ®åº“çš„æ‰€æœ‰è¡¨çŠ¶æ€"""
        status_file = self._get_status_file(tenant_id)
        
        if not status_file.exists():
            return {}
            
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥ {status_file}: {e}")
            return {}
    
    def _save_database_status(self, tenant_id: str, status_data: Dict):
        """ä¿å­˜æ•°æ®åº“çš„æ‰€æœ‰è¡¨çŠ¶æ€"""
        status_file = self._get_status_file(tenant_id)
        
        try:
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, indent=2, ensure_ascii=False)
            logger.info(f"  ðŸ’¾ æ›´æ–°æ•°æ®åº“çŠ¶æ€æ–‡ä»¶: {status_file.name}")
        except Exception as e:
            logger.error(f"âŒ å†™å…¥çŠ¶æ€æ–‡ä»¶å¤±è´¥ {status_file}: {e}")
    
    def get_last_sync_time(self, tenant_id: str, table_name: str) -> Optional[datetime]:
        """èŽ·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´"""
        with self._lock:
            db_status = self._load_database_status(tenant_id)
            table_status = db_status.get('tables', {}).get(table_name, {})
            
            if table_status.get('last_sync_time'):
                try:
                    return datetime.fromisoformat(table_status['last_sync_time'])
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£æžåŒæ­¥æ—¶é—´å¤±è´¥ {tenant_id}.{table_name}: {e}")
                    
        return None
    
    def update_sync_status(self, tenant_id: str, table_name: str,
                          sync_time: datetime, sync_mode: str, 
                          records_synced: int, status: str = 'SUCCESS', 
                          error_message: str = None):
        """æ›´æ–°åŒæ­¥çŠ¶æ€"""
        with self._lock:
            # åŠ è½½çŽ°æœ‰çŠ¶æ€
            db_status = self._load_database_status(tenant_id)
            
            # åˆå§‹åŒ–ç»“æž„
            if 'database_info' not in db_status:
                db_status['database_info'] = {
                    'tenant_id': tenant_id,
                    'last_updated': datetime.now().isoformat()
                }
            
            if 'tables' not in db_status:
                db_status['tables'] = {}
            
            # æ›´æ–°è¡¨çŠ¶æ€
            db_status['tables'][table_name] = {
                'table_name': table_name,
                'last_sync_time': sync_time.isoformat(),
                'sync_status': status,
                'sync_mode': sync_mode,
                'records_synced': records_synced,
                'error_message': error_message,
                'updated_at': datetime.now().isoformat()
            }
            
            # æ›´æ–°æ•°æ®åº“çº§åˆ«ä¿¡æ¯
            db_status['database_info']['last_updated'] = datetime.now().isoformat()
            db_status['database_info']['total_tables'] = len(db_status['tables'])
            
            # ä¿å­˜çŠ¶æ€
            self._save_database_status(tenant_id, db_status)
    
    def get_database_summary(self, tenant_id: str) -> Dict:
        """èŽ·å–æ•°æ®åº“åŒæ­¥æ‘˜è¦"""
        with self._lock:
            db_status = self._load_database_status(tenant_id)
            
            if not db_status:
                return {'tenant_id': tenant_id, 'total_tables': 0, 'tables': {}}
            
            return {
                'tenant_id': tenant_id,
                'database_info': db_status.get('database_info', {}),
                'total_tables': len(db_status.get('tables', {})),
                'tables': db_status.get('tables', {}),
                'last_updated': db_status.get('database_info', {}).get('last_updated')
            }

class TableAnalyzer:
    """è¡¨ç»“æž„åˆ†æžå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, connection_pool, cache: TableInfoCache):
        self.connection_pool = connection_pool
        self.cache = cache
    
    def get_table_info(self, db_name: str, table_name: str) -> Dict:
        """èŽ·å–è¡¨çš„å®Œæ•´ä¿¡æ¯ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        cached_info = self.cache.get_table_info(db_name, table_name)
        if cached_info:
            logger.info(f"  ðŸ’¾ ä½¿ç”¨ç¼“å­˜è¡¨ä¿¡æ¯: {db_name}.{table_name}")
            return cached_info
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
        logger.info(f"  ðŸ” åˆ†æžè¡¨ç»“æž„: {db_name}.{table_name}")
        
        conn = self.connection_pool.get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute(f"USE {db_name}")
            
            # ä¸€æ¬¡æ€§èŽ·å–æ‰€æœ‰è¡¨ä¿¡æ¯
            table_info = {
                'schema': [],
                'field_types': {},
                'timestamp_field': None,
                'primary_keys': []
            }
            
            # 1. èŽ·å–å­—æ®µä¿¡æ¯
            cursor.execute(f"DESCRIBE {table_name}")
            describe_results = cursor.fetchall()
            
            for field, ftype, *_ in describe_results:
                base_type = ftype.split("(")[0].lower()
                bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
                table_info['schema'].append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
                table_info['field_types'][field] = ftype.lower()
            
            # æ·»åŠ ç³»ç»Ÿå­—æ®µ
            table_info['schema'].extend([
                bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("sync_timestamp", "TIMESTAMP", mode="NULLABLE"),
                bigquery.SchemaField("sync_mode", "STRING", mode="NULLABLE")
            ])
            
            # 2. æ£€æµ‹æ—¶é—´æˆ³å­—æ®µ
            available_timestamp_fields = []
            for field, ftype in table_info['field_types'].items():
                field_lower = field.lower()
                if any(ts_field in field_lower for ts_field in ['time', 'date', 'created', 'updated', 'modified']):
                    if (any(ftype.startswith(dt) for dt in ['datetime', 'timestamp']) or
                        ('int' in ftype and any(kw in field_lower for kw in ['time', 'created', 'updated']))):
                        available_timestamp_fields.append((field, ftype))
            
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©æ—¶é—´æˆ³å­—æ®µ
            for preferred_field in TIMESTAMP_FIELDS:
                for available_field, field_type in available_timestamp_fields:
                    if preferred_field.lower() == available_field.lower():
                        table_info['timestamp_field'] = available_field
                        break
                if table_info['timestamp_field']:
                    break
            
            if not table_info['timestamp_field'] and available_timestamp_fields:
                table_info['timestamp_field'] = available_timestamp_fields[0][0]
            
            # 3. èŽ·å–ä¸»é”®ä¿¡æ¯
            cursor.execute(f"""
                SELECT COLUMN_NAME 
                FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE 
                WHERE TABLE_SCHEMA = '{db_name}' 
                AND TABLE_NAME = '{table_name}' 
                AND CONSTRAINT_NAME = 'PRIMARY'
                ORDER BY ORDINAL_POSITION
            """)
            table_info['primary_keys'] = [row[0] for row in cursor.fetchall()]
            
            cursor.close()
            
            # ç¼“å­˜ç»“æžœ
            self.cache.set_table_info(db_name, table_name, table_info)
            
            logger.info(f"  ðŸ“Š è¡¨åˆ†æžå®Œæˆ:")
            logger.info(f"    ðŸ• æ—¶é—´æˆ³å­—æ®µ: {table_info['timestamp_field'] or 'æ— '}")
            logger.info(f"    ðŸ”‘ ä¸»é”®å­—æ®µ: {table_info['primary_keys'] or 'æ— '}")
            logger.info(f"    ðŸ“‹ å­—æ®µæ•°é‡: {len(table_info['field_types'])}")
            
            return table_info
            
        finally:
            conn.close()

class BatchDataProcessor:
    """æ‰¹é‡æ•°æ®å¤„ç†å™¨"""
    
    @staticmethod
    def batch_normalize_data_types(rows: List[Dict], field_types: Dict[str, str]) -> List[Dict]:
        """æ‰¹é‡æ ‡å‡†åŒ–æ•°æ®ç±»åž‹"""
        if not rows:
            return rows
        
        logger.info(f"  ðŸ”„ æ‰¹é‡å¤„ç†æ•°æ®ç±»åž‹æ ‡å‡†åŒ–: {len(rows)} è¡Œ")
        
        # é¢„è®¡ç®—ç±»åž‹è½¬æ¢æ˜ å°„
        type_converters = {}
        for field, mysql_type in field_types.items():
            base_type = mysql_type.split("(")[0].lower()
            bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
            type_converters[field] = (bq_type, mysql_type)
        
        # æ‰¹é‡å¤„ç†
        normalized_rows = []
        conversion_stats = {}
        
        for row in rows:
            normalized_row = {}
            for key, value in row.items():
                if key in ['tenant_id', 'sync_timestamp', 'sync_mode']:
                    # ç³»ç»Ÿå­—æ®µä¿æŒä¸å˜
                    normalized_row[key] = value
                elif value is None:
                    normalized_row[key] = None
                elif key in type_converters:
                    # ä½¿ç”¨é¢„è®¡ç®—çš„è½¬æ¢å™¨
                    bq_type, mysql_type = type_converters[key]
                    old_value = value
                    normalized_row[key] = BatchDataProcessor._convert_value_to_bq_type(value, bq_type, mysql_type)
                    
                    # ç»Ÿè®¡è½¬æ¢
                    if key not in conversion_stats:
                        conversion_stats[key] = 0
                    if str(old_value) != str(normalized_row[key]):
                        conversion_stats[key] += 1
                else:
                    # æœªçŸ¥å­—æ®µè½¬ä¸ºå­—ç¬¦ä¸²
                    normalized_row[key] = str(value) if value is not None else None
            
            normalized_rows.append(normalized_row)
        
        # æ˜¾ç¤ºè½¬æ¢ç»Ÿè®¡
        for field, count in conversion_stats.items():
            if count > 0:
                bq_type, mysql_type = type_converters.get(field, ('STRING', 'unknown'))
                logger.info(f"    ðŸ”„ {field}: {mysql_type} â†’ {bq_type} ({count} æ¬¡è½¬æ¢)")
        
        logger.info(f"  âœ… æ‰¹é‡æ•°æ®ç±»åž‹æ ‡å‡†åŒ–å®Œæˆ")
        return normalized_rows
    
    @staticmethod
    def _convert_value_to_bq_type(value, bq_type: str, mysql_type: str):
        """å°†å€¼è½¬æ¢ä¸ºBigQueryå…¼å®¹çš„ç±»åž‹"""
        if value is None:
            return None
        
        try:
            if bq_type == "STRING":
                return str(value)
            elif bq_type == "INT64":
                return int(value) if value != '' else None
            elif bq_type == "FLOAT64":
                return float(value) if value != '' else None
            elif bq_type == "NUMERIC":
                return float(value) if value != '' else None
            elif bq_type == "BOOLEAN":
                if isinstance(value, bool):
                    return value
                elif isinstance(value, (int, str)):
                    return bool(int(value)) if str(value).isdigit() else bool(value)
                else:
                    return bool(value)
            elif bq_type == "TIMESTAMP":
                if isinstance(value, datetime):
                    return value.isoformat()
                elif isinstance(value, int) and 'time' in mysql_type:
                    # Unixæ—¶é—´æˆ³è½¬æ¢
                    return datetime.fromtimestamp(value).isoformat()
                else:
                    return str(value)
            elif bq_type == "DATE":
                if isinstance(value, datetime):
                    return value.date().isoformat()
                else:
                    return str(value)
            else:
                return str(value)
        except (ValueError, TypeError) as e:
            logger.warning(f"âš ï¸ ç±»åž‹è½¬æ¢å¤±è´¥ {value} -> {bq_type}: {e}, ä½¿ç”¨å­—ç¬¦ä¸²ç±»åž‹")
            return str(value)
    


class OptimizedIncrementalSyncer:
    """ä¼˜åŒ–ç‰ˆå¢žé‡åŒæ­¥å™¨"""
    
    def __init__(self, params: Dict):
        self.params = params
        
        # è§£æžæ•°æ®åº“åˆ—è¡¨
        db_names = [db.strip() for db in params['db_list'].split(",")]
        
        # åˆ›å»ºè¿žæŽ¥æ± ï¼ˆä¸æŒ‡å®šé»˜è®¤æ•°æ®åº“ï¼‰
        self.connection_pool = mysql.connector.pooling.MySQLConnectionPool(
            pool_name="sync_pool",
            pool_size=params.get('pool_size', 5),
            pool_reset_session=True,
            host=params['db_host'],
            port=int(params['db_port']),
            user=params['db_user'],
            password=params['db_pass']
        )
        logger.info(f"âœ… åˆ›å»ºè¿žæŽ¥æ± : {params.get('pool_size', 5)} ä¸ªè¿žæŽ¥")
        
        # åˆå§‹åŒ–ç¼“å­˜å’Œç»„ä»¶
        self.table_cache = TableInfoCache()
        self.status_manager = LocalFileStatusManager(params.get('status_dir', 'sync_status'))
        self.table_analyzer = TableAnalyzer(self.connection_pool, self.table_cache)
        self.bq_client = bigquery.Client(project=params['bq_project'])
        
        # é…ç½®å‚æ•°
        self.lookback_minutes = params.get('lookback_minutes', 10)
        self.batch_size = params.get('batch_size', 1000)
        self.max_retries = params.get('max_retries', 3)
        self.retry_delay = params.get('retry_delay', 5)
    
    def get_table_data(self, db_name: str, table_name: str, table_info: Dict, 
                      sync_mode: str, last_sync_time: datetime = None, 
                      current_sync_time: datetime = None) -> List[Dict]:
        """èŽ·å–è¡¨æ•°æ®ï¼ˆå¢žé‡æˆ–å…¨é‡ï¼‰"""
        conn = self.connection_pool.get_connection()
        try:
            cursor = conn.cursor(dictionary=True)
            cursor.execute(f"USE {db_name}")
            
            if sync_mode == 'INCREMENTAL' and last_sync_time and table_info['timestamp_field']:
                # å¢žé‡æŸ¥è¯¢
                timestamp_field = table_info['timestamp_field']
                timestamp_field_type = table_info['field_types'].get(timestamp_field, '').lower()
                
                # å®‰å…¨å›žé€€æ—¶é—´çª—å£
                safe_start_time = last_sync_time - timedelta(minutes=self.lookback_minutes)
                
                if 'int' in timestamp_field_type:
                    # Unixæ—¶é—´æˆ³æŸ¥è¯¢
                    safe_start_timestamp = int(safe_start_time.timestamp())
                    current_timestamp = int(current_sync_time.timestamp())
                    
                    query = f"""
                        SELECT * FROM {table_name} 
                        WHERE {timestamp_field} > %s 
                        AND {timestamp_field} <= %s
                        ORDER BY {timestamp_field} ASC
                    """
                    cursor.execute(query, (safe_start_timestamp, current_timestamp))
                    logger.info(f"  ðŸ” Unixæ—¶é—´æˆ³æŸ¥è¯¢: {timestamp_field} > {safe_start_timestamp} AND <= {current_timestamp}")
                else:
                    # æ—¥æœŸæ—¶é—´æŸ¥è¯¢
                    query = f"""
                        SELECT * FROM {table_name} 
                        WHERE {timestamp_field} > %s 
                        AND {timestamp_field} <= %s
                        ORDER BY {timestamp_field} ASC
                    """
                    cursor.execute(query, (safe_start_time, current_sync_time))
                    logger.info(f"  ðŸ” æ—¥æœŸæ—¶é—´æŸ¥è¯¢: {timestamp_field} > {safe_start_time} AND <= {current_sync_time}")
            else:
                # å…¨é‡æŸ¥è¯¢
                cursor.execute(f"SELECT * FROM {table_name}")
                logger.info(f"  ðŸ” å…¨é‡æ•°æ®æŸ¥è¯¢")
            
            rows = cursor.fetchall()
            cursor.close()
            
            if not rows:
                logger.info(f"  â„¹ï¸ æ— æ•°æ®è¿”å›ž")
                return []
            
            logger.info(f"  ðŸ“¥ èŽ·å–æ•°æ®: {len(rows)} è¡Œ")
            
            # æ‰¹é‡æ·»åŠ ç³»ç»Ÿå­—æ®µ
            for row in rows:
                row['tenant_id'] = db_name
                row['sync_timestamp'] = current_sync_time.isoformat()
                row['sync_mode'] = sync_mode
                
                # åŸºç¡€ç±»åž‹å¤„ç†
                for key, value in row.items():
                    if isinstance(value, datetime):
                        row[key] = value.isoformat()
                    elif isinstance(value, Decimal):
                        row[key] = float(value)
            
            # æ‰¹é‡æ•°æ®å¤„ç†
            rows = BatchDataProcessor.batch_normalize_data_types(rows, table_info['field_types'])
            
            return rows
            
        finally:
            conn.close()
    
    def ensure_bq_table(self, table_name: str, schema: List[bigquery.SchemaField]):
        """ç¡®ä¿BigQueryè¡¨å­˜åœ¨"""
        dataset_id = self.params['bq_dataset']
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
        try:
            self.bq_client.get_dataset(dataset_id)
        except:
            dataset = bigquery.Dataset(f"{self.params['bq_project']}.{dataset_id}")
            dataset.location = "US"
            self.bq_client.create_dataset(dataset)
            logger.info(f"ðŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
        
        # åˆ›å»ºè¡¨ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰- æ‰€æœ‰ç§Ÿæˆ·å…±äº«åŒä¸€ä¸ªè¡¨
        table_id = f"{self.params['bq_project']}.{dataset_id}.{table_name}"
        try:
            self.bq_client.get_table(table_id)
        except:
            table = bigquery.Table(table_id, schema=schema)
            # è®¾ç½®åˆ†åŒºå’Œèšç°‡
            table.time_partitioning = bigquery.TimePartitioning(
                type_=bigquery.TimePartitioningType.DAY,
                field="sync_timestamp"
            )
            table.clustering_fields = ["tenant_id"]
            table = self.bq_client.create_table(table)
            logger.info(f"ðŸ†• åˆ›å»ºè¡¨: {table_name} (å¤šç§Ÿæˆ·å…±äº«)")
    
    def write_to_bigquery(self, table_name: str, rows: List[Dict], 
                         schema: List[bigquery.SchemaField], 
                         primary_keys: List[str], sync_mode: str):
        """å†™å…¥BigQuery"""
        if not rows:
            return
        
        table_id = f"{self.params['bq_project']}.{self.params['bq_dataset']}.{table_name}"
        
        if sync_mode == 'FULL':
            # å…¨é‡åŒæ­¥ï¼šå…ˆåˆ é™¤è¯¥ç§Ÿæˆ·çš„æ•°æ®ï¼Œå†æ’å…¥æ–°æ•°æ®
            tenant_id = rows[0]['tenant_id'] if rows else None
            if tenant_id:
                # åˆ é™¤è¯¥ç§Ÿæˆ·çš„çŽ°æœ‰æ•°æ®
                delete_sql = f"""
                DELETE FROM `{table_id}` 
                WHERE tenant_id = '{tenant_id}'
                """
                delete_job = self.bq_client.query(delete_sql)
                delete_job.result()
                logger.info(f"ðŸ—‘ï¸ å·²åˆ é™¤ç§Ÿæˆ· {tenant_id} çš„çŽ°æœ‰æ•°æ®")
            
            # æ’å…¥æ–°æ•°æ®
            job_config = bigquery.LoadJobConfig(
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                schema=schema
            )
            job = self.bq_client.load_table_from_json(rows, table_id, job_config=job_config)
            job.result()
            logger.info(f"âœ… å…¨é‡å†™å…¥å®Œæˆ: {len(rows)} è¡Œ (ç§Ÿæˆ·: {tenant_id})")
            
        else:
            # å¢žé‡åŒæ­¥ï¼šä¼˜å…ˆä½¿ç”¨MERGEæ“ä½œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            if primary_keys:
                # æœ‰ä¸»é”®ï¼šä½¿ç”¨MERGEæ“ä½œï¼ˆæ”¯æŒæ’å…¥å’Œæ›´æ–°ï¼‰
                self._merge_data(table_id, rows, primary_keys, schema)
                logger.info(f"âœ… MERGEæ“ä½œå®Œæˆ: {len(rows)} è¡Œ")
            else:
                # æ— ä¸»é”®ï¼šä½¿ç”¨APPENDæ¨¡å¼ï¼ˆä»…è¿½åŠ ï¼‰
                job_config = bigquery.LoadJobConfig(
                    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                    schema=schema
                )
                job = self.bq_client.load_table_from_json(rows, table_id, job_config=job_config)
                job.result()
                logger.info(f"âœ… å¢žé‡è¿½åŠ å®Œæˆ: {len(rows)} è¡Œï¼ˆæ— ä¸»é”®ï¼Œä»…è¿½åŠ ï¼‰")
    
    def _merge_data(self, table_id: str, rows: List[Dict], primary_keys: List[str], schema: List[bigquery.SchemaField]):
        """ä½¿ç”¨MERGEæ“ä½œæ›´æ–°æ•°æ®"""
        # åˆ›å»ºä¸´æ—¶è¡¨
        temp_table_id = f"{table_id}_temp_{int(time.time())}"
        
        # ä¸Šä¼ æ•°æ®åˆ°ä¸´æ—¶è¡¨ï¼Œä½¿ç”¨ä¸Žç›®æ ‡è¡¨ç›¸åŒçš„schema
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
            schema=schema
        )
        job = self.bq_client.load_table_from_json(rows, temp_table_id, job_config=job_config)
        job.result()
        
        # æž„å»ºMERGE SQL
        pk_conditions = " AND ".join([f"T.{pk} = S.{pk}" for pk in primary_keys])
        pk_conditions += " AND T.tenant_id = S.tenant_id"
        
        # èŽ·å–æ‰€æœ‰å­—æ®µï¼ˆé™¤äº†ä¸»é”®å’Œç³»ç»Ÿå­—æ®µï¼‰
        sample_row = rows[0]
        update_fields = []
        insert_fields = []
        insert_values = []
        
        for field in sample_row.keys():
            # æ‰€æœ‰å­—æ®µéƒ½å‚ä¸ŽINSERT
            insert_fields.append(field)
            insert_values.append(f"S.{field}")
            
            # UPDATEæ—¶æŽ’é™¤ä¸»é”®å­—æ®µï¼ˆä¸»é”®ä¸èƒ½è¢«æ›´æ–°ï¼‰
            if field not in primary_keys:
                update_fields.append(f"{field} = S.{field}")
        
        merge_sql = f"""
        MERGE `{table_id}` T
        USING `{temp_table_id}` S
        ON {pk_conditions}
        WHEN MATCHED THEN
          UPDATE SET {', '.join(update_fields)}
        WHEN NOT MATCHED THEN
          INSERT ({', '.join(insert_fields)})
          VALUES ({', '.join(insert_values)})
        """
        
        # æ‰§è¡ŒMERGE
        query_job = self.bq_client.query(merge_sql)
        query_job.result()
        
        # åˆ é™¤ä¸´æ—¶è¡¨
        self.bq_client.delete_table(temp_table_id)
        
        logger.info(f"âœ… MERGEæ“ä½œå®Œæˆ: {len(rows)} è¡Œ")
    
    def sync_table(self, db_name: str, table_name: str, force_full: bool = False) -> Dict:
        """åŒæ­¥å•ä¸ªè¡¨"""
        logger.info(f"\nðŸš€ å¼€å§‹åŒæ­¥è¡¨: {db_name}.{table_name}")
        
        current_sync_time = datetime.now()
        sync_stats = {
            'tenant_id': db_name,
            'table_name': table_name,
            'sync_mode': 'FULL',
            'records_synced': 0,
            'status': 'SUCCESS',
            'error_message': None,
            'start_time': current_sync_time
        }
        
        try:
            # èŽ·å–è¡¨ä¿¡æ¯ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            table_info = self.table_analyzer.get_table_info(db_name, table_name)
            
            # ç¡®ä¿BigQueryè¡¨å­˜åœ¨
            self.ensure_bq_table(table_name, table_info['schema'])
            
            # å†³å®šåŒæ­¥æ¨¡å¼
            last_sync_time = None if force_full else self.status_manager.get_last_sync_time(db_name, table_name)
            
            if last_sync_time and table_info['timestamp_field'] and not force_full:
                # å¢žé‡åŒæ­¥
                sync_stats['sync_mode'] = 'INCREMENTAL'
                logger.info(f"ðŸ”„ æ‰§è¡Œå¢žé‡åŒæ­¥ï¼Œä¸Šæ¬¡åŒæ­¥æ—¶é—´: {last_sync_time}")
                
                rows = self.get_table_data(
                    db_name, table_name, table_info, 'INCREMENTAL',
                    last_sync_time, current_sync_time
                )
            else:
                # å…¨é‡åŒæ­¥
                sync_stats['sync_mode'] = 'FULL'
                reason = "å¼ºåˆ¶å…¨é‡" if force_full else ("é¦–æ¬¡åŒæ­¥" if not last_sync_time else "æ— æ—¶é—´æˆ³å­—æ®µ")
                logger.info(f"ðŸ”„ æ‰§è¡Œå…¨é‡åŒæ­¥ï¼ŒåŽŸå› : {reason}")
                
                rows = self.get_table_data(
                    db_name, table_name, table_info, 'FULL',
                    current_sync_time=current_sync_time
                )
            
            # å†™å…¥BigQuery
            if rows:
                self.write_to_bigquery(
                    table_name, rows, table_info['schema'], 
                    table_info['primary_keys'], sync_stats['sync_mode']
                )
                sync_stats['records_synced'] = len(rows)
                logger.info(f"âœ… åŒæ­¥å®Œæˆ: {len(rows)} è¡Œæ•°æ®")
            else:
                logger.info("â„¹ï¸ æ— æ–°æ•°æ®éœ€è¦åŒæ­¥")
            
            # æ›´æ–°åŒæ­¥çŠ¶æ€
            self.status_manager.update_sync_status(
                db_name, table_name, current_sync_time, 
                sync_stats['sync_mode'], sync_stats['records_synced']
            )
            
        except Exception as e:
            sync_stats['status'] = 'FAILED'
            sync_stats['error_message'] = str(e)
            logger.error(f"âŒ åŒæ­¥å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            self.status_manager.update_sync_status(
                db_name, table_name, current_sync_time, 
                sync_stats['sync_mode'], 0, 'FAILED', str(e)
            )
        
        sync_stats['end_time'] = datetime.now()
        sync_stats['duration'] = (sync_stats['end_time'] - sync_stats['start_time']).total_seconds()
        
        return sync_stats
    
    def sync_database_parallel(self, db_name: str, table_names: List[str], force_full: bool = False) -> List[Dict]:
        """å¹¶è¡ŒåŒæ­¥å•ä¸ªæ•°æ®åº“çš„æ‰€æœ‰è¡¨"""
        logger.info(f"ðŸ“‚ å¹¶è¡Œå¤„ç†æ•°æ®åº“: {db_name} ({len(table_names)} å¼ è¡¨)")
        
        database_stats = []
        max_workers = min(len(table_names), 3)  # æœ€å¤š3ä¸ªå¹¶å‘çº¿ç¨‹
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰è¡¨çš„åŒæ­¥ä»»åŠ¡
            future_to_table = {
                executor.submit(self.sync_table_safe, db_name, table_name, force_full): table_name
                for table_name in table_names
            }
            
            # æ”¶é›†ç»“æžœ
            for future in as_completed(future_to_table):
                table_name = future_to_table[future]
                try:
                    table_stats = future.result()
                    table_stats['database'] = db_name
                    table_stats['table'] = table_name
                    table_stats['status'] = 'SUCCESS'
                    database_stats.append(table_stats)
                    logger.info(f"âœ… è¡¨åŒæ­¥å®Œæˆ: {db_name}.{table_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ è¡¨åŒæ­¥å¤±è´¥: {db_name}.{table_name}, é”™è¯¯: {e}")
                    database_stats.append({
                        'database': db_name,
                        'table': table_name,
                        'status': 'FAILED',
                        'error_message': str(e),
                        'records_synced': 0,
                        'duration': 0,
                        'sync_mode': 'UNKNOWN'
                    })
        
        return database_stats
    
    def sync_table_safe(self, db_name: str, table_name: str, force_full: bool = False) -> Dict:
        """çº¿ç¨‹å®‰å…¨çš„è¡¨åŒæ­¥æ–¹æ³•"""
        thread_id = threading.current_thread().ident
        logger.info(f"ðŸš€ [çº¿ç¨‹{thread_id}] å¼€å§‹åŒæ­¥è¡¨: {db_name}.{table_name}")
        
        try:
            return self.sync_table(db_name, table_name, force_full)
        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹{thread_id}] è¡¨åŒæ­¥å¼‚å¸¸: {db_name}.{table_name}, é”™è¯¯: {e}")
            raise
    
    def sync_all_tables(self, force_full: bool = False) -> Dict:
        """å®‰å…¨å¹¶è¡ŒåŒæ­¥æ‰€æœ‰è¡¨"""
        logger.info("ðŸš€ å¼€å§‹å¹¶è¡Œæ™ºèƒ½å¢žé‡åŒæ­¥ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ")
        logger.info("=" * 60)
        
        # è§£æžé…ç½®
        db_names = [db.strip() for db in self.params['db_list'].split(",")]
        table_names = [t.strip() for t in self.params['table_list'].split(",")]
        
        logger.info(f"ðŸŽ¯ æ•°æ®åº“: {db_names}")
        logger.info(f"ðŸ“‹ è¡¨å: {table_names}")
        logger.info(f"ðŸ“Š ç›®æ ‡: {self.params['bq_project']}.{self.params['bq_dataset']}")
        logger.info(f"ðŸ”§ åŒæ­¥æ¨¡å¼: {'å¼ºåˆ¶å…¨é‡' if force_full else 'æ™ºèƒ½å¢žé‡'}")
        logger.info(f"âš¡ æ€§èƒ½ä¼˜åŒ–: è¿žæŽ¥æ± ({self.params.get('pool_size', 5)}) + è¡¨ç»“æž„ç¼“å­˜ + æ‰¹é‡å¤„ç† + å¹¶è¡ŒåŒæ­¥")
        
        # åŒæ­¥ç»Ÿè®¡
        total_stats = {
            'total_tables': len(db_names) * len(table_names),
            'success_count': 0,
            'failed_count': 0,
            'full_sync_count': 0,
            'incremental_sync_count': 0,
            'total_records': 0,
            'start_time': datetime.now(),
            'table_stats': []
        }
        
        # æ•°æ®åº“çº§ä¸²è¡Œå¤„ç†ï¼Œè¡¨çº§å¹¶è¡Œå¤„ç†ï¼ˆå®‰å…¨æ–¹æ¡ˆï¼‰
        for db_name in db_names:
            logger.info(f"ðŸ“‚ å¼€å§‹å¤„ç†æ•°æ®åº“: {db_name}")
            db_start_time = datetime.now()
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ•°æ®åº“çš„æ‰€æœ‰è¡¨
            database_stats = self.sync_database_parallel(db_name, table_names, force_full)
            
            # æ±‡æ€»æ•°æ®åº“ç»Ÿè®¡
            for table_stat in database_stats:
                total_stats['table_stats'].append(table_stat)
                
                if table_stat['status'] == 'SUCCESS':
                    total_stats['success_count'] += 1
                    total_stats['total_records'] += table_stat.get('records_synced', 0)
                    
                    if table_stat.get('sync_mode') == 'FULL':
                        total_stats['full_sync_count'] += 1
                    else:
                        total_stats['incremental_sync_count'] += 1
                else:
                    total_stats['failed_count'] += 1
            
            db_duration = (datetime.now() - db_start_time).total_seconds()
            db_records = sum(stat.get('records_synced', 0) for stat in database_stats if stat['status'] == 'SUCCESS')
            logger.info(f"âœ… æ•°æ®åº“å¤„ç†å®Œæˆ: {db_name} ({db_records} è¡Œ, {db_duration:.1f}ç§’)")
        
        total_stats['end_time'] = datetime.now()
        total_stats['total_duration'] = (total_stats['end_time'] - total_stats['start_time']).total_seconds()
        
        # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
        self._print_sync_report(total_stats)
        
        return total_stats
    
    def _print_sync_report(self, stats: Dict):
        """æ‰“å°åŒæ­¥æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("ðŸ“Š æ™ºèƒ½å¢žé‡åŒæ­¥ç»Ÿè®¡æŠ¥å‘Š - æ€§èƒ½ä¼˜åŒ–ç‰ˆ")
        logger.info("=" * 60)
        logger.info(f"ðŸ“‹ æ€»è¡¨æ•°: {stats['total_tables']}")
        logger.info(f"âœ… æˆåŠŸè¡¨æ•°: {stats['success_count']}")
        logger.info(f"âŒ å¤±è´¥è¡¨æ•°: {stats['failed_count']}")
        logger.info(f"ðŸ“ˆ æ€»å¤„ç†è¡Œæ•°: {stats['total_records']:,}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {stats['total_duration']:.2f} ç§’")
        
        if stats['total_records'] > 0 and stats['total_duration'] > 0:
            throughput = stats['total_records'] / stats['total_duration']
            logger.info(f"ðŸš€ å¤„ç†é€Ÿåº¦: {throughput:.1f} è¡Œ/ç§’")
        
        logger.info(f"\nðŸŽ¯ åŒæ­¥æ¨¡å¼ç»Ÿè®¡:")
        logger.info(f"  ðŸ”„ å…¨é‡åŒæ­¥: {stats['full_sync_count']} å¼ è¡¨")
        logger.info(f"  âš¡ å¢žé‡åŒæ­¥: {stats['incremental_sync_count']} å¼ è¡¨")
        
        logger.info(f"\nâš¡ æ€§èƒ½ä¼˜åŒ–æ•ˆæžœ:")
        logger.info(f"  ðŸ’¾ è¡¨ç»“æž„ç¼“å­˜å‘½ä¸­: {len(self.table_cache._cache)} å¼ è¡¨")
        logger.info(f"  ðŸ”— è¿žæŽ¥æ± å¤ç”¨: å‡å°‘è¿žæŽ¥å»ºç«‹å¼€é”€")
        logger.info(f"  ðŸ“¦ æ‰¹é‡æ•°æ®å¤„ç†: æå‡å¤„ç†æ•ˆçŽ‡")
        logger.info(f"  ðŸš€ å¹¶è¡ŒåŒæ­¥: æ•°æ®åº“ä¸²è¡Œ + è¡¨çº§å¹¶è¡Œï¼ˆå®‰å…¨æ¨¡å¼ï¼‰")
        
        if stats['failed_count'] > 0:
            logger.info(f"\nâŒ å¤±è´¥è¡¨è¯¦æƒ…:")
            for table_stat in stats['table_stats']:
                if table_stat['status'] == 'FAILED':
                    logger.info(f"  ðŸ“‹ {table_stat['tenant_id']}.{table_stat['table_name']}: {table_stat['error_message']}")
        
        logger.info("\nðŸŽ‰ åŒæ­¥å®Œæˆï¼")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.table_cache.clear()
            # è¿žæŽ¥æ± ä¼šè‡ªåŠ¨ç®¡ç†è¿žæŽ¥
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1 and sys.argv[1] == '--full':
        force_full = True
        print("ðŸ”„ å¼ºåˆ¶å…¨é‡åŒæ­¥æ¨¡å¼")
    else:
        force_full = False
        print("âš¡ æ™ºèƒ½å¢žé‡åŒæ­¥æ¨¡å¼")
    
    # è¯»å–é…ç½®
    try:
        with open('params.json', 'r') as f:
            params = json.load(f)
    except FileNotFoundError:
        logger.error("âŒ é…ç½®æ–‡ä»¶ params.json ä¸å­˜åœ¨")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error("âŒ é…ç½®æ–‡ä»¶ params.json æ ¼å¼é”™è¯¯")
        sys.exit(1)
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆåŒæ­¥å™¨å¹¶æ‰§è¡ŒåŒæ­¥
    syncer = OptimizedIncrementalSyncer(params)
    
    try:
        stats = syncer.sync_all_tables(force_full=force_full)
        
        # æ ¹æ®ç»“æžœè®¾ç½®é€€å‡ºç 
        if stats['failed_count'] > 0:
            sys.exit(1)
        else:
            sys.exit(0)
    finally:
        syncer.cleanup()

if __name__ == "__main__":
    main()