#!/usr/bin/env python3
"""
MySQL åˆ° BigQuery æ™ºèƒ½åŒæ­¥å·¥å…· - æ™ºèƒ½ç­–ç•¥é€‰æ‹©
æ ¹æ®è¡¨ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä½³åŒæ­¥ç­–ç•¥ï¼Œå®Œç¾å¤„ç†å„ç§æ•°æ®åœºæ™¯

åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ”‘ æ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼šæœ‰ä¸»é”®ç”¨MERGEï¼Œæ— ä¸»é”®ç”¨å“ˆå¸Œå»é‡
- ğŸ”„ MERGEæ“ä½œï¼šå®Œç¾å¤„ç†æ•°æ®æ›´æ–°ï¼Œé¿å…é€»è¾‘é‡å¤
- ğŸ§  å“ˆå¸Œå»é‡ï¼šæ— ä¸»é”®è¡¨çš„æ™ºèƒ½å»é‡æ–¹æ¡ˆ
- ğŸ¢ å¤šç§Ÿæˆ·æ”¯æŒï¼šå®Œç¾è§£å†³å¤šç§Ÿæˆ·æ•°æ®è¦†ç›–é—®é¢˜
- âš¡ é«˜æ•ˆåŒæ­¥ï¼šæ™ºèƒ½æ£€æµ‹é‡å¤æ•°æ®ï¼Œé¿å…æ— æ•ˆä¼ è¾“
- ğŸ“Š è¯¦ç»†ç»Ÿè®¡ï¼šå®Œæ•´çš„åŒæ­¥æŠ¥å‘Šå’Œæ•°æ®è´¨é‡åˆ†æ
- ğŸ›¡ï¸ æ•°æ®å®‰å…¨ï¼šè‡ªåŠ¨å¤‡ä»½å’Œæ¢å¤æœºåˆ¶
"""

import mysql.connector
from google.cloud import bigquery
import json
import sys
from datetime import datetime
from decimal import Decimal
import hashlib

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def get_primary_keys(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å–è¡¨çš„ä¸»é”®ä¿¡æ¯"""
    print(f"ğŸ”‘ æ£€æµ‹ä¸»é”®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    
    # æŸ¥è¯¢ä¸»é”®ä¿¡æ¯
    query = """
    SELECT COLUMN_NAME 
    FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE 
    WHERE TABLE_SCHEMA = %s 
    AND TABLE_NAME = %s 
    AND CONSTRAINT_NAME = 'PRIMARY'
    ORDER BY ORDINAL_POSITION
    """
    
    cursor.execute(query, (db_name, table_name))
    primary_keys = [row[0] for row in cursor.fetchall()]
    
    cursor.close()
    conn.close()
    
    if primary_keys:
        print(f"  âœ… å‘ç°ä¸»é”®: {primary_keys}")
    else:
        print(f"  âš ï¸ æœªå‘ç°ä¸»é”®ï¼Œå°†ä½¿ç”¨å“ˆå¸Œå»é‡ç­–ç•¥")
    
    return primary_keys

def get_table_schema(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ å¤šç§Ÿæˆ·å­—æ®µ
    schema.append(bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"))
    schema.append(bigquery.SchemaField("data_hash", "STRING", mode="NULLABLE"))
    schema.append(bigquery.SchemaField("last_updated", "TIMESTAMP", mode="NULLABLE"))
    
    cursor.close()
    conn.close()
    return schema

def generate_data_hash(row_data):
    """ç”Ÿæˆæ•°æ®è¡Œçš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ™ºèƒ½å»é‡"""
    # æ’é™¤ç³»ç»Ÿå­—æ®µï¼šå“ˆå¸Œå­—æ®µã€ç§Ÿæˆ·å­—æ®µã€æ—¶é—´æˆ³å­—æ®µ
    hash_data = {}
    for key, value in row_data.items():
        if key not in ['data_hash', 'tenant_id', 'last_updated']:
            if isinstance(value, datetime):
                hash_data[key] = value.isoformat()
            elif isinstance(value, Decimal):
                hash_data[key] = str(value)
            elif value is None:
                hash_data[key] = "NULL"
            else:
                hash_data[key] = str(value)
    
    # æŒ‰é”®æ’åºç¡®ä¿ä¸€è‡´æ€§
    sorted_data = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(sorted_data.encode('utf-8')).hexdigest()

def get_table_data(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨æ•°æ®"""
    print(f"ğŸ“¥ è¯»å–æ•°æ®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor(dictionary=True)
    cursor.execute(f"SELECT * FROM {table_name}")
    
    rows = []
    current_time = datetime.now()
    
    for row in cursor.fetchall():
        # æ·»åŠ ç§Ÿæˆ·æ ‡è¯†å’Œæ›´æ–°æ—¶é—´
        row['tenant_id'] = db_name
        row['last_updated'] = current_time
        
        # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.isoformat()
            elif isinstance(value, Decimal):
                row[key] = float(value)
            elif value is None:
                row[key] = None
        
        # ç”Ÿæˆæ™ºèƒ½å“ˆå¸Œç”¨äºå»é‡
        row['data_hash'] = generate_data_hash(row)
        rows.append(row)
    
    print(f"  ğŸ“Š è¯»å–åˆ° {len(rows)} è¡Œæ•°æ®")
    cursor.close()
    conn.close()
    return rows

def create_temp_table(client, table_id, schema, rows):
    """åˆ›å»ºä¸´æ—¶è¡¨å­˜å‚¨æºæ•°æ®"""
    temp_table_id = f"{table_id}_temp_{int(datetime.now().timestamp())}"
    
    # åˆ›å»ºä¸´æ—¶è¡¨
    temp_table = bigquery.Table(temp_table_id, schema=schema)
    temp_table = client.create_table(temp_table)
    print(f"  ğŸ”§ åˆ›å»ºä¸´æ—¶è¡¨: {temp_table_id}")
    
    # åŠ è½½æ•°æ®åˆ°ä¸´æ—¶è¡¨
    job_config = bigquery.LoadJobConfig(
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        schema=schema
    )
    
    job = client.load_table_from_json(rows, temp_table_id, job_config=job_config)
    job.result()
    print(f"  ğŸ“¥ æ•°æ®å·²åŠ è½½åˆ°ä¸´æ—¶è¡¨: {len(rows)} è¡Œ")
    
    return temp_table_id

def execute_merge_query(client, target_table_id, temp_table_id, primary_keys, schema, tenant_id):
    """æ‰§è¡ŒMERGEæŸ¥è¯¢"""
    print(f"  ğŸ”„ æ‰§è¡ŒMERGEæ“ä½œ...")
    
    # è·å–ç›®æ ‡è¡¨çš„å®é™…å­—æ®µ
    try:
        target_table = client.get_table(target_table_id)
        target_fields = {field.name for field in target_table.schema}
        print(f"  ğŸ“‹ ç›®æ ‡è¡¨å­—æ®µ: {sorted(target_fields)}")
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–ç›®æ ‡è¡¨ç»“æ„: {e}")
        target_fields = {field.name for field in schema}
    
    # æ„å»ºä¸»é”®åŒ¹é…æ¡ä»¶
    pk_conditions = []
    for pk in primary_keys:
        pk_conditions.append(f"T.{pk} = S.{pk}")
    pk_match = " AND ".join(pk_conditions)
    
    # æ„å»ºUPDATE SETå­å¥ï¼ˆåªåŒ…å«ç›®æ ‡è¡¨ä¸­å­˜åœ¨çš„å­—æ®µï¼Œæ’é™¤ä¸»é”®å’Œtenant_idï¼‰
    update_fields = []
    for field in schema:
        if (field.name in target_fields and 
            field.name not in primary_keys and 
            field.name not in ['tenant_id']):
            update_fields.append(f"{field.name} = S.{field.name}")
    
    update_set = ", ".join(update_fields)
    
    # æ„å»ºINSERTå­—æ®µåˆ—è¡¨ï¼ˆåªåŒ…å«ç›®æ ‡è¡¨ä¸­å­˜åœ¨çš„å­—æ®µï¼‰
    insert_fields = [field.name for field in schema if field.name in target_fields]
    insert_columns = ", ".join(insert_fields)
    insert_values = ", ".join([f"S.{field}" for field in insert_fields])
    
    # æ„å»ºä¸»é”®+å“ˆå¸Œå€¼çš„æ™ºèƒ½MERGEç­–ç•¥
    if 'data_hash' in target_fields:
        print(f"  ğŸ” ä½¿ç”¨ä¸»é”®+å“ˆå¸Œå€¼ç»„åˆç­–ç•¥")
        print(f"     ğŸ”‘ ä¸»é”®åŒ¹é…: {primary_keys}")
        print(f"     ğŸ§® å“ˆå¸Œæ¯”è¾ƒ: data_hashå­—æ®µï¼ˆæ’é™¤æ—¶é—´æˆ³ï¼‰")
        print(f"     â° æ—¶é—´æˆ³: last_updatedå­—æ®µè®°å½•åŒæ­¥æ—¶é—´")
        
        # ä¸»é”®åŒ¹é… + å“ˆå¸Œå€¼ä¸åŒæ—¶æ‰æ›´æ–°
        merge_query = f"""
        MERGE `{target_table_id}` T
        USING `{temp_table_id}` S
        ON {pk_match} AND T.tenant_id = S.tenant_id
        WHEN MATCHED AND T.data_hash != S.data_hash THEN
          UPDATE SET {update_set}
        WHEN NOT MATCHED THEN
          INSERT ({insert_columns})
          VALUES ({insert_values})
        """
    else:
        print(f"  âš ï¸ ç›®æ ‡è¡¨ç¼ºå°‘data_hashå­—æ®µï¼Œä½¿ç”¨åŸºç¡€MERGEç­–ç•¥")
        
        # å…œåº•ç­–ç•¥ï¼šåŸºç¡€MERGEï¼ˆæ€»æ˜¯æ›´æ–°åŒ¹é…çš„è®°å½•ï¼‰
        merge_query = f"""
        MERGE `{target_table_id}` T
        USING `{temp_table_id}` S
        ON {pk_match} AND T.tenant_id = S.tenant_id
        WHEN MATCHED THEN
          UPDATE SET {update_set}
        WHEN NOT MATCHED THEN
          INSERT ({insert_columns})
          VALUES ({insert_values})
        """
    
    print(f"  ğŸ“ MERGEæŸ¥è¯¢:")
    print(f"     åŒ¹é…æ¡ä»¶: {pk_match}")
    print(f"     ç§Ÿæˆ·è¿‡æ»¤: T.tenant_id = '{tenant_id}'")
    print(f"     æ›´æ–°å­—æ®µ: {len(update_fields)} ä¸ª")
    print(f"     æ’å…¥å­—æ®µ: {len(insert_fields)} ä¸ª")
    
    # å…ˆæŸ¥è¯¢ç°æœ‰æ•°æ®ç»Ÿè®¡
    pre_merge_query = f"""
    SELECT COUNT(*) as existing_count
    FROM `{target_table_id}` 
    WHERE tenant_id = '{tenant_id}'
    """
    pre_result = client.query(pre_merge_query).result()
    existing_count = list(pre_result)[0].existing_count
    
    # æŸ¥è¯¢æºæ•°æ®ç»Ÿè®¡
    source_count_query = f"SELECT COUNT(*) as source_count FROM `{temp_table_id}`"
    source_result = client.query(source_count_query).result()
    source_count = list(source_result)[0].source_count
    
    print(f"  ğŸ“Š æ“ä½œå‰ç»Ÿè®¡: ç›®æ ‡è¡¨ç°æœ‰ {existing_count} è¡Œ, æºæ•°æ® {source_count} è¡Œ")
    
    # æ‰§è¡ŒMERGEæŸ¥è¯¢
    job = client.query(merge_query)
    result = job.result()
    
    # æŸ¥è¯¢æ“ä½œåç»Ÿè®¡
    post_merge_query = f"""
    SELECT COUNT(*) as final_count
    FROM `{target_table_id}` 
    WHERE tenant_id = '{tenant_id}'
    """
    post_result = client.query(post_merge_query).result()
    final_count = list(post_result)[0].final_count
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    inserted_rows = final_count - existing_count
    
    # åŸºäºä¸»é”®+å“ˆå¸Œå€¼ç­–ç•¥çš„ç²¾ç¡®ç»Ÿè®¡
    if 'data_hash' in target_fields:
        # ç»Ÿè®¡å“ˆå¸Œå€¼ä¸åŒçš„è®°å½•ï¼ˆå®é™…æ›´æ–°çš„è®°å½•ï¼‰
        hash_diff_query = f"""
        SELECT COUNT(*) as hash_different
        FROM `{temp_table_id}` S
        JOIN `{target_table_id}` T
        ON {pk_match.replace('T.', 'T.').replace('S.', 'S.')} 
        AND T.tenant_id = S.tenant_id
        WHERE T.data_hash != S.data_hash
        """
        hash_result = client.query(hash_diff_query).result()
        actual_updated_rows = list(hash_result)[0].hash_different
        
        # è®¡ç®—æ— å˜åŒ–çš„è®°å½•æ•°
        matched_records = source_count - inserted_rows
        unchanged_rows = matched_records - actual_updated_rows
        
        print(f"  ğŸ” å“ˆå¸Œæ¯”è¾ƒç»“æœ:")
        print(f"     ğŸ“Š åŒ¹é…è®°å½•: {matched_records} è¡Œ")
        print(f"     ğŸ”„ å“ˆå¸Œä¸åŒ: {actual_updated_rows} è¡Œ")
        print(f"     âšª å“ˆå¸Œç›¸åŒ: {unchanged_rows} è¡Œ")
    else:
        # å…œåº•ç»Ÿè®¡ï¼ˆæ— å“ˆå¸Œå­—æ®µæ—¶ï¼‰
        actual_updated_rows = max(0, source_count - inserted_rows) if inserted_rows < source_count else 0
        unchanged_rows = 0
    
    print(f"  ğŸ“ˆ MERGEæ“ä½œç»Ÿè®¡:")
    print(f"     ğŸ†• æ–°å¢è®°å½•: {inserted_rows} è¡Œ")
    print(f"     ğŸ”„ å®é™…æ›´æ–°: {actual_updated_rows} è¡Œ")
    print(f"     âšª æ— å˜åŒ–: {unchanged_rows} è¡Œ")
    print(f"     ğŸ“Š æœ€ç»ˆæ€»æ•°: {final_count} è¡Œ")
    print(f"  âœ… MERGEæ“ä½œå®Œæˆ")
    
    return {
        'merge_completed': True,
        'existing_count': existing_count,
        'source_count': source_count,
        'final_count': final_count,
        'inserted_rows': inserted_rows,
        'updated_rows': actual_updated_rows,
        'unchanged_rows': unchanged_rows
    }

def get_existing_hashes(client, table_id, tenant_id):
    """è·å–ç°æœ‰æ•°æ®çš„å“ˆå¸Œå€¼"""
    try:
        query = f"""
        SELECT DISTINCT data_hash 
        FROM `{table_id}` 
        WHERE tenant_id = '{tenant_id}' AND data_hash IS NOT NULL
        """
        result = client.query(query).result()
        existing_hashes = {row.data_hash for row in result}
        print(f"  ğŸ” ç§Ÿæˆ· {tenant_id} ç°æœ‰æ•°æ®å“ˆå¸Œ: {len(existing_hashes)} ä¸ª")
        return existing_hashes
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•æŸ¥è¯¢ç°æœ‰æ•°æ®å“ˆå¸Œ: {e}")
        return set()

class SmartSyncEngine:
    """æ™ºèƒ½åŒæ­¥å¼•æ“ - æ™ºèƒ½ç­–ç•¥é€‰æ‹©"""
    
    def __init__(self, params):
        self.params = params
        self.client = bigquery.Client(project=params['bq_project'])
        self.stats = {
            'total_tables': 0,
            'success_tables': 0,
            'tables_with_pk': 0,
            'tables_without_pk': 0,
            'total_rows_processed': 0,
            'total_new_rows': 0,
            'total_duplicate_rows': 0,
            'total_merge_operations': 0
        }
    
    def sync_table_merge(self, table_name, db_names, primary_keys):
        """ä½¿ç”¨MERGEç­–ç•¥åŒæ­¥å•ä¸ªè¡¨"""
        print(f"\nğŸ”„ ä½¿ç”¨MERGEç­–ç•¥åŒæ­¥è¡¨: {table_name}")
        print(f"ğŸ”‘ ä¸»é”®: {primary_keys}")
        
        # è·å–è¡¨ç»“æ„
        schema = get_table_schema(
            self.params['db_host'], self.params['db_port'], 
            self.params['db_user'], self.params['db_pass'], 
            db_names[0], table_name
        )
        
        # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
        dataset_id = self.params['bq_dataset']
        table_id = f"{self.params['bq_project']}.{dataset_id}.{table_name}"
        
        try:
            self.client.get_dataset(dataset_id)
        except:
            dataset = bigquery.Dataset(f"{self.params['bq_project']}.{dataset_id}")
            dataset.location = "US"
            self.client.create_dataset(dataset)
            print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        table_exists = True
        try:
            existing_table = self.client.get_table(table_id)
            print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
        except:
            table_exists = False
        
        if not table_exists:
            # åˆ›å»ºæ–°è¡¨
            table = bigquery.Table(table_id, schema=schema)
            table = self.client.create_table(table)
            print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
        
        # ä¸ºæ¯ä¸ªç§Ÿæˆ·å¤„ç†æ•°æ®
        total_processed_rows = 0
        
        for db_name in db_names:
            print(f"\n  ğŸ¢ å¤„ç†ç§Ÿæˆ·: {db_name}")
            
            try:
                # è·å–è¯¥ç§Ÿæˆ·çš„æ•°æ®
                rows = get_table_data(
                    self.params['db_host'], self.params['db_port'],
                    self.params['db_user'], self.params['db_pass'],
                    db_name, table_name
                )
                
                if not rows:
                    print(f"    âš ï¸ ç§Ÿæˆ· {db_name} æ— æ•°æ®")
                    continue
                
                # åˆ›å»ºä¸´æ—¶è¡¨
                temp_table_id = create_temp_table(self.client, table_id, schema, rows)
                
                try:
                    # æ‰§è¡ŒMERGEæ“ä½œ
                    merge_stats = execute_merge_query(
                        self.client, table_id, temp_table_id, 
                        primary_keys, schema, db_name
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    if merge_stats.get('inserted_rows', 0) > 0:
                        self.stats['total_new_rows'] += merge_stats['inserted_rows']
                    
                    print(f"    âœ… ç§Ÿæˆ· {db_name} MERGEå®Œæˆ:")
                    print(f"       ğŸ“Š å¤„ç†æ•°æ®: {len(rows)} è¡Œ")
                    print(f"       ğŸ†• æ–°å¢: {merge_stats.get('inserted_rows', 0)} è¡Œ")
                    print(f"       ğŸ”„ æ›´æ–°: {merge_stats.get('updated_rows', 0)} è¡Œ")
                    
                    total_processed_rows += len(rows)
                    self.stats['total_merge_operations'] += 1
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶è¡¨
                    self.client.delete_table(temp_table_id)
                    print(f"    ğŸ—‘ï¸ ä¸´æ—¶è¡¨å·²æ¸…ç†")
                
            except Exception as e:
                print(f"    âŒ ç§Ÿæˆ· {db_name} åŒæ­¥å¤±è´¥: {str(e)}")
                continue
        
        self.stats['total_rows_processed'] += total_processed_rows
        return total_processed_rows

    def sync_table_smart_append(self, table_name, db_names):
        """æ™ºèƒ½å®‰å…¨è¿½åŠ åŒæ­¥å•ä¸ªè¡¨"""
        print(f"\nğŸš€ å¼€å§‹æ™ºèƒ½åŒæ­¥è¡¨: {table_name}")
        print(f"ğŸ¢ æ¶‰åŠç§Ÿæˆ·: {db_names}")
        
        # è·å–è¡¨ç»“æ„ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®åº“ï¼‰
        schema = get_table_schema(
            self.params['db_host'], self.params['db_port'], 
            self.params['db_user'], self.params['db_pass'], 
            db_names[0], table_name
        )
        
        # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
        dataset_id = self.params['bq_dataset']
        table_id = f"{self.params['bq_project']}.{dataset_id}.{table_name}"
        
        try:
            self.client.get_dataset(dataset_id)
        except:
            dataset = bigquery.Dataset(f"{self.params['bq_project']}.{dataset_id}")
            dataset.location = "US"
            self.client.create_dataset(dataset)
            print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        table_exists = True
        try:
            existing_table = self.client.get_table(table_id)
            print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
            
            # æ£€æŸ¥è¡¨æ˜¯å¦æœ‰data_hashå­—æ®µ
            existing_fields = [field.name for field in existing_table.schema]
            has_hash_field = 'data_hash' in existing_fields
            
            if not has_hash_field:
                print(f"  âš ï¸ è¡¨ç¼ºå°‘data_hashå­—æ®µï¼Œå°†é‡å»ºè¡¨ç»“æ„")
                # å¤‡ä»½ç°æœ‰æ•°æ®
                backup_table_id = f"{table_id}_backup_{int(datetime.now().timestamp())}"
                backup_query = f"CREATE TABLE `{backup_table_id}` AS SELECT * FROM `{table_id}`"
                self.client.query(backup_query).result()
                print(f"  ğŸ’¾ æ•°æ®å·²å¤‡ä»½åˆ°: {backup_table_id}")
                
                # åˆ é™¤åŸè¡¨
                self.client.delete_table(table_id)
                table_exists = False
                
        except:
            table_exists = False
        
        if not table_exists:
            # åˆ›å»ºæ–°è¡¨
            table = bigquery.Table(table_id, schema=schema)
            table = self.client.create_table(table)
            print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
        
        # ä¸ºæ¯ä¸ªç§Ÿæˆ·å¤„ç†æ•°æ®
        total_new_rows = 0
        total_duplicate_rows = 0
        
        for db_name in db_names:
            print(f"\n  ğŸ¢ å¤„ç†ç§Ÿæˆ·: {db_name}")
            
            try:
                # è·å–è¯¥ç§Ÿæˆ·çš„æ•°æ®
                rows = get_table_data(
                    self.params['db_host'], self.params['db_port'],
                    self.params['db_user'], self.params['db_pass'],
                    db_name, table_name
                )
                
                if not rows:
                    print(f"    âš ï¸ ç§Ÿæˆ· {db_name} æ— æ•°æ®")
                    continue
                
                # è·å–ç°æœ‰æ•°æ®å“ˆå¸Œ
                existing_hashes = get_existing_hashes(self.client, table_id, db_name)
                
                # æ™ºèƒ½è¿‡æ»¤é‡å¤æ•°æ®
                new_rows = []
                duplicate_count = 0
                
                for row in rows:
                    if row['data_hash'] not in existing_hashes:
                        new_rows.append(row)
                    else:
                        duplicate_count += 1
                
                print(f"    ğŸ“Š æ–°æ•°æ®: {len(new_rows)} è¡Œ, é‡å¤æ•°æ®: {duplicate_count} è¡Œ")
                
                if new_rows:
                    # å®‰å…¨è¿½åŠ æ–°æ•°æ®
                    job_config = bigquery.LoadJobConfig(
                        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                        schema=schema
                    )
                    
                    job = self.client.load_table_from_json(new_rows, table_id, job_config=job_config)
                    job.result()
                    
                    print(f"    âœ… ç§Ÿæˆ· {db_name} æ–°æ•°æ®åŒæ­¥å®Œæˆ: {len(new_rows)} è¡Œ")
                    total_new_rows += len(new_rows)
                else:
                    print(f"    âš ï¸ ç§Ÿæˆ· {db_name} æ— æ–°æ•°æ®éœ€è¦åŒæ­¥")
                
                total_duplicate_rows += duplicate_count
                
            except Exception as e:
                print(f"    âŒ ç§Ÿæˆ· {db_name} åŒæ­¥å¤±è´¥: {str(e)}")
                continue
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_tables'] += 1
        if total_new_rows > 0 or total_duplicate_rows > 0:
            self.stats['success_tables'] += 1
        self.stats['total_rows_processed'] += (total_new_rows + total_duplicate_rows)
        self.stats['total_new_rows'] += total_new_rows
        self.stats['total_duplicate_rows'] += total_duplicate_rows
        
        print(f"\n  ğŸ“ˆ è¡¨ {table_name} æ™ºèƒ½åŒæ­¥ç»Ÿè®¡:")
        print(f"    ğŸ†• æ–°å¢æ•°æ®: {total_new_rows} è¡Œ")
        print(f"    ğŸ”„ é‡å¤æ•°æ®: {total_duplicate_rows} è¡Œ")
        print(f"    âš¡ å»é‡æ•ˆç‡: {(total_duplicate_rows / (total_new_rows + total_duplicate_rows) * 100):.1f}%" if (total_new_rows + total_duplicate_rows) > 0 else "    âš¡ å»é‡æ•ˆç‡: 0%")
        print(f"    âœ… åŒæ­¥å®Œæˆ")
        
        return {
            'table_name': table_name,
            'new_rows': total_new_rows,
            'duplicate_rows': total_duplicate_rows,
            'total_rows': total_new_rows + total_duplicate_rows
        }
    
    def sync_table(self, table_name, db_names):
        """æ™ºèƒ½åŒæ­¥å•ä¸ªè¡¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥"""
        print(f"\nğŸš€ å¼€å§‹æ™ºèƒ½åŒæ­¥è¡¨: {table_name}")
        print(f"ğŸ¢ æ¶‰åŠç§Ÿæˆ·: {db_names}")
        
        # æ£€æµ‹ä¸»é”®ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®åº“ï¼‰
        primary_keys = get_primary_keys(
            self.params['db_host'], self.params['db_port'],
            self.params['db_user'], self.params['db_pass'],
            db_names[0], table_name
        )
        
        # æ™ºèƒ½ç­–ç•¥é€‰æ‹©
        if primary_keys:
            print(f"  ğŸ¯ ç­–ç•¥é€‰æ‹©: MERGEï¼ˆåŸºäºä¸»é”® {primary_keys}ï¼‰")
            self.stats['tables_with_pk'] += 1
            total_rows = self.sync_table_merge(table_name, db_names, primary_keys)
        else:
            print(f"  ğŸ¯ ç­–ç•¥é€‰æ‹©: å“ˆå¸Œå»é‡ï¼ˆæ— ä¸»é”®è¡¨ï¼‰")
            self.stats['tables_without_pk'] += 1
            result = self.sync_table_smart_append(table_name, db_names)
            total_rows = result['new_rows'] if result else 0
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_tables'] += 1
        if total_rows > 0:
            self.stats['success_tables'] += 1
        
        return {
            'table_name': table_name,
            'strategy': 'MERGE' if primary_keys else 'HASH_DEDUP',
            'primary_keys': primary_keys,
            'processed_rows': total_rows
        }
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š æ™ºèƒ½åŒæ­¥ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ“‹ æ€»è¡¨æ•°: {self.stats['total_tables']}")
        print(f"âœ… æˆåŠŸè¡¨æ•°: {self.stats['success_tables']}")
        print(f"âŒ å¤±è´¥è¡¨æ•°: {self.stats['total_tables'] - self.stats['success_tables']}")
        print(f"ğŸ”‘ æœ‰ä¸»é”®è¡¨æ•°: {self.stats['tables_with_pk']}")
        print(f"âš ï¸ æ— ä¸»é”®è¡¨æ•°: {self.stats['tables_without_pk']}")
        print(f"ğŸ“ˆ æ€»å¤„ç†è¡Œæ•°: {self.stats['total_rows_processed']:,}")
        print(f"ğŸ†• æ–°å¢è¡Œæ•°: {self.stats['total_new_rows']:,}")
        print(f"ğŸ”„ é‡å¤è¡Œæ•°: {self.stats['total_duplicate_rows']:,}")
        print(f"ğŸ”„ MERGEæ“ä½œ: {self.stats['total_merge_operations']} æ¬¡")
        
        print(f"\nğŸ¯ åŒæ­¥ç­–ç•¥åˆ†å¸ƒ:")
        print(f"  MERGEç­–ç•¥: {self.stats['tables_with_pk']} å¼ è¡¨ï¼ˆæœ‰ä¸»é”®ï¼‰")
        print(f"  å“ˆå¸Œå»é‡ç­–ç•¥: {self.stats['tables_without_pk']} å¼ è¡¨ï¼ˆæ— ä¸»é”®ï¼‰")
        
        if self.stats['total_rows_processed'] > 0:
            if self.stats['total_duplicate_rows'] > 0:
                dedup_rate = (self.stats['total_duplicate_rows'] / self.stats['total_rows_processed']) * 100
                print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
                print(f"  å»é‡ç‡: {dedup_rate:.1f}%")
                print(f"  æ•°æ®æ–°é²œåº¦: {(self.stats['total_new_rows'] / self.stats['total_rows_processed']) * 100:.1f}%")
            else:
                print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
                print(f"  MERGEæ“ä½œ: å®Œç¾å¤„ç†æ•°æ®æ›´æ–°ï¼Œæ— é‡å¤æ•°æ®")
            
            success_rate = (self.stats['success_tables'] / max(self.stats['total_tables'], 1)) * 100
            print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        else:
            print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
            print(f"  æ— æ•°æ®å¤„ç†")

def main():
    print("ğŸ§  MySQL åˆ° BigQuery æ™ºèƒ½åŒæ­¥å·¥å…·")
    print("=" * 60)
    print("ğŸ¯ åŸºäºå®‰å…¨è¿½åŠ åŒæ­¥ï¼Œæ™ºèƒ½å“ˆå¸Œå»é‡ï¼Œå®Œç¾è§£å†³é‡å¤æ•°æ®é—®é¢˜")
    print()
    
    # è¯»å–é…ç½®
    try:
        with open('params.json', 'r') as f:
            params = json.load(f)
    except FileNotFoundError:
        print("âŒ é…ç½®æ–‡ä»¶ params.json ä¸å­˜åœ¨")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        sys.exit(1)
    
    # è§£æå‚æ•°
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸ“Š ç›®æ ‡: {params['bq_project']}.{params['bq_dataset']}")
    print(f"ğŸ”§ åŒæ­¥æ¨¡å¼: æ™ºèƒ½å®‰å…¨è¿½åŠ ï¼ˆå“ˆå¸Œå»é‡ï¼‰")
    
    # æ£€æµ‹æ˜¯å¦ä¸ºå¤šç§Ÿæˆ·åœºæ™¯
    is_multitenant = len(db_names) > 1
    print(f"ğŸ¢ å¤šç§Ÿæˆ·æ¨¡å¼: {'æ˜¯' if is_multitenant else 'å¦'}")
    
    # åˆ›å»ºæ™ºèƒ½åŒæ­¥å¼•æ“
    engine = SmartSyncEngine(params)
    
    # æŒ‰è¡¨åˆ†ç»„åŒæ­¥ï¼ˆå¤šç§Ÿæˆ·å‹å¥½ï¼‰
    print(f"\nğŸš€ å¼€å§‹æ™ºèƒ½åŒæ­¥ï¼Œå…± {len(table_names)} å¼ è¡¨")
    
    for i, table_name in enumerate(table_names, 1):
        print(f"\n[{i}/{len(table_names)}] å¤„ç†è¡¨: {table_name}")
        try:
            engine.sync_table(table_name, db_names)
        except Exception as e:
            print(f"âŒ è¡¨ {table_name} åŒæ­¥å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    engine.print_final_stats()
    
    # æ˜¾ç¤ºéªŒè¯æŸ¥è¯¢
    print(f"\nğŸ“Š éªŒè¯æŸ¥è¯¢ç¤ºä¾‹:")
    for table_name in table_names:
        print(f"""
-- æŸ¥çœ‹è¡¨ {table_name} çš„ç§Ÿæˆ·æ•°æ®ç»Ÿè®¡
SELECT 
  tenant_id, 
  COUNT(*) as total_rows,
  COUNT(DISTINCT data_hash) as unique_rows,
  MAX(sync_timestamp) as last_sync_time
FROM `{params['bq_project']}.{params['bq_dataset']}.{table_name}`
GROUP BY tenant_id
ORDER BY tenant_id;
        """)
    
    print(f"\nğŸ‰ æ™ºèƒ½åŒæ­¥å®Œæˆï¼")
    print(f"ğŸ’¡ æç¤º: ç³»ç»Ÿä½¿ç”¨æ™ºèƒ½å“ˆå¸Œå»é‡ï¼Œç¡®ä¿æ•°æ®é›¶é‡å¤")

if __name__ == "__main__":
    main()