#!/usr/bin/env python3
"""
çŠ¶æ€æ–‡ä»¶è¿ç§»å·¥å…·
å°†å•è¡¨çŠ¶æ€æ–‡ä»¶åˆå¹¶ä¸ºæŒ‰æ•°æ®åº“åˆ†ç»„çš„çŠ¶æ€æ–‡ä»¶
"""

import json
import os
from pathlib import Path
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def migrate_status_files(status_dir: str = "sync_status"):
    """è¿ç§»çŠ¶æ€æ–‡ä»¶"""
    status_path = Path(status_dir)
    
    if not status_path.exists():
        logger.info("çŠ¶æ€ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€è¿ç§»")
        return
    
    # æ‰«ææ‰€æœ‰å•è¡¨çŠ¶æ€æ–‡ä»¶
    single_table_files = list(status_path.glob("*_*.json"))
    
    if not single_table_files:
        logger.info("æœªæ‰¾åˆ°éœ€è¦è¿ç§»çš„å•è¡¨çŠ¶æ€æ–‡ä»¶")
        return
    
    logger.info(f"å‘ç° {len(single_table_files)} ä¸ªå•è¡¨çŠ¶æ€æ–‡ä»¶ï¼Œå¼€å§‹è¿ç§»...")
    
    # æŒ‰æ•°æ®åº“åˆ†ç»„
    databases = {}
    
    for file_path in single_table_files:
        try:
            # è§£ææ–‡ä»¶åï¼štenant_id_table_name.json
            filename = file_path.stem
            parts = filename.split('_')
            
            if len(parts) < 2:
                logger.warning(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„æ–‡ä»¶: {file_path.name}")
                continue
            
            # æ‰¾åˆ°tenant_idå’Œtable_nameçš„åˆ†ç•Œç‚¹
            # å‡è®¾tenant_idä¸åŒ…å«ä¸‹åˆ’çº¿ï¼Œtable_nameå¯èƒ½åŒ…å«ä¸‹åˆ’çº¿
            tenant_id = parts[0]
            table_name = '_'.join(parts[1:])
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                table_data = json.load(f)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(table_data, dict) or 'table_name' not in table_data:
                logger.warning(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„æ•°æ®æ–‡ä»¶: {file_path.name}")
                continue
            
            # æŒ‰æ•°æ®åº“åˆ†ç»„
            if tenant_id not in databases:
                databases[tenant_id] = {
                    'database_info': {
                        'tenant_id': tenant_id,
                        'last_updated': datetime.now().isoformat()
                    },
                    'tables': {}
                }
            
            # æ·»åŠ è¡¨æ•°æ®
            databases[tenant_id]['tables'][table_name] = {
                'table_name': table_data.get('table_name', table_name),
                'last_sync_time': table_data.get('last_sync_time'),
                'sync_status': table_data.get('sync_status', 'UNKNOWN'),
                'sync_mode': table_data.get('sync_mode', 'UNKNOWN'),
                'records_synced': table_data.get('records_synced', 0),
                'error_message': table_data.get('error_message'),
                'updated_at': table_data.get('updated_at', datetime.now().isoformat())
            }
            
            logger.info(f"  âœ… è¿ç§»è¡¨: {tenant_id}.{table_name}")
            
        except Exception as e:
            logger.error(f"  âŒ è¿ç§»å¤±è´¥ {file_path.name}: {e}")
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®åº“çŠ¶æ€æ–‡ä»¶
    migrated_count = 0
    for tenant_id, db_data in databases.items():
        try:
            # æ›´æ–°æ•°æ®åº“ä¿¡æ¯
            db_data['database_info']['total_tables'] = len(db_data['tables'])
            
            # ä¿å­˜æ–°æ ¼å¼æ–‡ä»¶
            new_file_path = status_path / f"{tenant_id}.json"
            with open(new_file_path, 'w', encoding='utf-8') as f:
                json.dump(db_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… åˆ›å»ºæ•°æ®åº“çŠ¶æ€æ–‡ä»¶: {new_file_path.name} ({len(db_data['tables'])} å¼ è¡¨)")
            migrated_count += 1
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®åº“çŠ¶æ€æ–‡ä»¶å¤±è´¥ {tenant_id}: {e}")
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•å¹¶ç§»åŠ¨æ—§æ–‡ä»¶
    if migrated_count > 0:
        backup_dir = status_path / "backup_single_table_files"
        backup_dir.mkdir(exist_ok=True)
        
        moved_count = 0
        for file_path in single_table_files:
            try:
                backup_path = backup_dir / file_path.name
                file_path.rename(backup_path)
                moved_count += 1
            except Exception as e:
                logger.warning(f"âš ï¸ ç§»åŠ¨å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        
        logger.info(f"ğŸ“¦ å·²å°† {moved_count} ä¸ªæ—§æ–‡ä»¶ç§»åŠ¨åˆ°å¤‡ä»½ç›®å½•: {backup_dir}")
    
    logger.info(f"ğŸ‰ è¿ç§»å®Œæˆï¼æˆåŠŸè¿ç§» {migrated_count} ä¸ªæ•°æ®åº“çš„çŠ¶æ€æ–‡ä»¶")

def preview_migration(status_dir: str = "sync_status"):
    """é¢„è§ˆè¿ç§»ç»“æœ"""
    status_path = Path(status_dir)
    
    if not status_path.exists():
        logger.info("çŠ¶æ€ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ‰«æå•è¡¨æ–‡ä»¶
    single_table_files = list(status_path.glob("*_*.json"))
    
    # æ‰«ææ•°æ®åº“æ–‡ä»¶ï¼ˆæ’é™¤åŒ…å«ä¸‹åˆ’çº¿çš„æ–‡ä»¶ï¼‰
    database_files = [f for f in status_path.glob("*.json") 
                     if '_' not in f.stem and f in status_path.iterdir()]
    
    logger.info("ğŸ“Š å½“å‰çŠ¶æ€æ–‡ä»¶æ¦‚è§ˆ:")
    logger.info(f"  ğŸ“„ å•è¡¨æ–‡ä»¶: {len(single_table_files)} ä¸ª")
    logger.info(f"  ğŸ—„ï¸ æ•°æ®åº“æ–‡ä»¶: {len(database_files)} ä¸ª")
    
    if single_table_files:
        logger.info("\nğŸ“„ å•è¡¨æ–‡ä»¶åˆ—è¡¨:")
        databases_preview = {}
        for file_path in single_table_files:
            filename = file_path.stem
            parts = filename.split('_')
            if len(parts) >= 2:
                tenant_id = parts[0]
                table_name = '_'.join(parts[1:])
                
                if tenant_id not in databases_preview:
                    databases_preview[tenant_id] = []
                databases_preview[tenant_id].append(table_name)
        
        for tenant_id, tables in databases_preview.items():
            logger.info(f"  ğŸª {tenant_id}: {len(tables)} å¼ è¡¨ ({', '.join(tables[:3])}{'...' if len(tables) > 3 else ''})")
    
    if database_files:
        logger.info("\nğŸ—„ï¸ æ•°æ®åº“æ–‡ä»¶åˆ—è¡¨:")
        for file_path in database_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    table_count = len(data.get('tables', {}))
                    last_updated = data.get('database_info', {}).get('last_updated', 'æœªçŸ¥')
                    logger.info(f"  ğŸª {file_path.stem}: {table_count} å¼ è¡¨ (æ›´æ–°: {last_updated})")
            except Exception as e:
                logger.warning(f"  âš ï¸ è¯»å–å¤±è´¥ {file_path.name}: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--preview':
        preview_migration()
    else:
        migrate_status_files()