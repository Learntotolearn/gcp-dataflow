#!/usr/bin/env python3
"""
MySQL åˆ° BigQuery å®‰å…¨è¿½åŠ åŒæ­¥å·¥å…·
è§£å†³APPENDæ¨¡å¼é‡å¤æ•°æ®é—®é¢˜ï¼Œä½¿ç”¨ç®€å•æœ‰æ•ˆçš„å»é‡ç­–ç•¥
"""

import mysql.connector
from google.cloud import bigquery
import json
import sys
from datetime import datetime
from decimal import Decimal
import hashlib

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def get_table_schema(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ  tenant_id å­—æ®µ
    schema.append(bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"))
    # å¢åŠ æ•°æ®å“ˆå¸Œå­—æ®µç”¨äºå»é‡
    schema.append(bigquery.SchemaField("data_hash", "STRING", mode="NULLABLE"))
    # å¢åŠ åŒæ­¥æ—¶é—´æˆ³
    schema.append(bigquery.SchemaField("sync_timestamp", "TIMESTAMP", mode="NULLABLE"))
    
    cursor.close()
    conn.close()
    return schema

def generate_data_hash(row_data):
    """ç”Ÿæˆæ•°æ®è¡Œçš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡"""
    # æ’é™¤å“ˆå¸Œå­—æ®µå’ŒåŒæ­¥æ—¶é—´æˆ³
    hash_data = {}
    for key, value in row_data.items():
        if key not in ['data_hash', 'sync_timestamp']:
            if isinstance(value, datetime):
                hash_data[key] = value.isoformat()
            elif isinstance(value, Decimal):
                hash_data[key] = str(value)
            elif value is None:
                hash_data[key] = "NULL"
            else:
                hash_data[key] = str(value)
    
    # æŒ‰é”®æ’åºç¡®ä¿ä¸€è‡´æ€§
    sorted_data = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(sorted_data.encode('utf-8')).hexdigest()

def get_table_data(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨æ•°æ®"""
    print(f"ğŸ“¥ è¯»å–æ•°æ®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor(dictionary=True)
    cursor.execute(f"SELECT * FROM {table_name}")
    
    rows = []
    current_time = datetime.now()
    
    for row in cursor.fetchall():
        # æ·»åŠ  tenant_id
        row['tenant_id'] = db_name
        # æ·»åŠ åŒæ­¥æ—¶é—´æˆ³
        row['sync_timestamp'] = current_time
        
        # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.isoformat()
            elif isinstance(value, Decimal):
                row[key] = float(value)
            elif value is None:
                row[key] = None
        
        # ç”Ÿæˆæ•°æ®å“ˆå¸Œ
        row['data_hash'] = generate_data_hash(row)
        rows.append(row)
    
    print(f"  ğŸ“Š è¯»å–åˆ° {len(rows)} è¡Œæ•°æ®")
    cursor.close()
    conn.close()
    return rows

def get_existing_hashes(client, table_id, tenant_id):
    """è·å–ç°æœ‰æ•°æ®çš„å“ˆå¸Œå€¼"""
    try:
        query = f"""
        SELECT DISTINCT data_hash 
        FROM `{table_id}` 
        WHERE tenant_id = '{tenant_id}' AND data_hash IS NOT NULL
        """
        result = client.query(query).result()
        existing_hashes = {row.data_hash for row in result}
        print(f"  ğŸ” ç§Ÿæˆ· {tenant_id} ç°æœ‰æ•°æ®å“ˆå¸Œ: {len(existing_hashes)} ä¸ª")
        return existing_hashes
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•æŸ¥è¯¢ç°æœ‰æ•°æ®å“ˆå¸Œ: {e}")
        return set()

def sync_table_safe_append(params, table_name, db_names):
    """å®‰å…¨è¿½åŠ åŒæ­¥å•ä¸ªè¡¨"""
    print(f"\nğŸš€ å¼€å§‹å®‰å…¨è¿½åŠ åŒæ­¥è¡¨: {table_name}")
    print(f"ğŸ¢ æ¶‰åŠç§Ÿæˆ·: {db_names}")
    
    # è·å–è¡¨ç»“æ„ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®åº“ï¼‰
    schema = get_table_schema(
        params['db_host'], params['db_port'], 
        params['db_user'], params['db_pass'], 
        db_names[0], table_name
    )
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
    dataset_id = params['bq_dataset']
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    
    try:
        client.get_dataset(dataset_id)
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    table_exists = True
    try:
        existing_table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦æœ‰data_hashå­—æ®µ
        existing_fields = [field.name for field in existing_table.schema]
        has_hash_field = 'data_hash' in existing_fields
        
        if not has_hash_field:
            print(f"  âš ï¸ è¡¨ç¼ºå°‘data_hashå­—æ®µï¼Œå°†é‡å»ºè¡¨ç»“æ„")
            # å¤‡ä»½ç°æœ‰æ•°æ®
            backup_table_id = f"{table_id}_backup_{int(datetime.now().timestamp())}"
            backup_query = f"CREATE TABLE `{backup_table_id}` AS SELECT * FROM `{table_id}`"
            client.query(backup_query).result()
            print(f"  ğŸ’¾ æ•°æ®å·²å¤‡ä»½åˆ°: {backup_table_id}")
            
            # åˆ é™¤åŸè¡¨
            client.delete_table(table_id)
            table_exists = False
            
    except:
        table_exists = False
    
    if not table_exists:
        # åˆ›å»ºæ–°è¡¨
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
    
    # ä¸ºæ¯ä¸ªç§Ÿæˆ·å¤„ç†æ•°æ®
    total_new_rows = 0
    total_duplicate_rows = 0
    
    for db_name in db_names:
        print(f"\n  ğŸ¢ å¤„ç†ç§Ÿæˆ·: {db_name}")
        
        try:
            # è·å–è¯¥ç§Ÿæˆ·çš„æ•°æ®
            rows = get_table_data(
                params['db_host'], params['db_port'],
                params['db_user'], params['db_pass'],
                db_name, table_name
            )
            
            if not rows:
                print(f"    âš ï¸ ç§Ÿæˆ· {db_name} æ— æ•°æ®")
                continue
            
            # è·å–ç°æœ‰æ•°æ®å“ˆå¸Œ
            existing_hashes = get_existing_hashes(client, table_id, db_name)
            
            # è¿‡æ»¤é‡å¤æ•°æ®
            new_rows = []
            duplicate_count = 0
            
            for row in rows:
                if row['data_hash'] not in existing_hashes:
                    new_rows.append(row)
                else:
                    duplicate_count += 1
            
            print(f"    ğŸ“Š æ–°æ•°æ®: {len(new_rows)} è¡Œ, é‡å¤æ•°æ®: {duplicate_count} è¡Œ")
            
            if new_rows:
                # è¿½åŠ æ–°æ•°æ®
                job_config = bigquery.LoadJobConfig(
                    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                    schema=schema
                )
                
                job = client.load_table_from_json(new_rows, table_id, job_config=job_config)
                job.result()
                
                print(f"    âœ… ç§Ÿæˆ· {db_name} æ–°æ•°æ®åŒæ­¥å®Œæˆ: {len(new_rows)} è¡Œ")
                total_new_rows += len(new_rows)
            else:
                print(f"    âš ï¸ ç§Ÿæˆ· {db_name} æ— æ–°æ•°æ®éœ€è¦åŒæ­¥")
            
            total_duplicate_rows += duplicate_count
            
        except Exception as e:
            print(f"    âŒ ç§Ÿæˆ· {db_name} åŒæ­¥å¤±è´¥: {str(e)}")
            continue
    
    print(f"\n  ğŸ“ˆ è¡¨ {table_name} åŒæ­¥ç»Ÿè®¡:")
    print(f"    ğŸ†• æ–°å¢æ•°æ®: {total_new_rows} è¡Œ")
    print(f"    ğŸ”„ é‡å¤æ•°æ®: {total_duplicate_rows} è¡Œ")
    print(f"    âœ… åŒæ­¥å®Œæˆ")

def main():
    print("ğŸš€ MySQL åˆ° BigQuery å®‰å…¨è¿½åŠ åŒæ­¥å·¥å…·")
    print("=" * 60)
    
    # è¯»å–é…ç½®
    with open('params.json', 'r') as f:
        params = json.load(f)
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸ“Š ç›®æ ‡: {params['bq_project']}.{params['bq_dataset']}")
    print(f"ğŸ”§ åŒæ­¥æ¨¡å¼: å®‰å…¨è¿½åŠ ï¼ˆå»é‡ï¼‰")
    
    # æ£€æµ‹æ˜¯å¦ä¸ºå¤šç§Ÿæˆ·åœºæ™¯
    is_multitenant = len(db_names) > 1
    print(f"ğŸ¢ å¤šç§Ÿæˆ·æ¨¡å¼: {'æ˜¯' if is_multitenant else 'å¦'}")
    
    # æŒ‰è¡¨åˆ†ç»„åŒæ­¥
    success_count = 0
    for i, table_name in enumerate(table_names, 1):
        print(f"\n[{i}/{len(table_names)}] å¤„ç†è¡¨: {table_name}")
        try:
            sync_table_safe_append(params, table_name, db_names)
            success_count += 1
        except Exception as e:
            print(f"âŒ è¡¨ {table_name} åŒæ­¥å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    print(f"\nğŸ‰ å®‰å…¨è¿½åŠ åŒæ­¥å®Œæˆï¼æˆåŠŸå¤„ç†äº† {success_count}/{len(table_names)} å¼ è¡¨")
    
    # æ˜¾ç¤ºéªŒè¯æŸ¥è¯¢
    print(f"\nğŸ“Š éªŒè¯æŸ¥è¯¢ç¤ºä¾‹:")
    for table_name in table_names:
        print(f"""
-- æŸ¥çœ‹è¡¨ {table_name} çš„ç§Ÿæˆ·æ•°æ®ç»Ÿè®¡
SELECT 
  tenant_id, 
  COUNT(*) as total_rows,
  COUNT(DISTINCT data_hash) as unique_rows,
  MAX(sync_timestamp) as last_sync_time
FROM `{params['bq_project']}.{params['bq_dataset']}.{table_name}`
GROUP BY tenant_id
ORDER BY tenant_id;
        """)

if __name__ == "__main__":
    main()