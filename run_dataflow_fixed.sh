#!/bin/bash
# åŸºäºæˆåŠŸçš„ simple_sync_fixed.py çš„ Dataflow äº‘ç«¯è¿è¡Œè„šæœ¬

echo "â˜ï¸ Dataflow äº‘ç«¯è¿è¡Œ - MySQL åˆ° BigQuery åŒæ­¥"
echo "================================================"

# æ£€æŸ¥å¿…è¦æ–‡ä»¶
if [ ! -f "simple_sync_fixed.py" ]; then
    echo "âŒ é”™è¯¯: simple_sync_fixed.py æ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi

if [ ! -f "params.json" ]; then
    echo "âŒ é”™è¯¯: params.json é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi

# è¯»å–é…ç½®
PROJECT_ID=$(python3 -c "import json; print(json.load(open('params.json'))['bq_project'])")
REGION="us-central1"
JOB_NAME="mysql-to-bq-sync-$(date +%Y%m%d-%H%M%S)"
TEMP_LOCATION="gs://ttpos-dataflow-templates/tmp/"
STAGING_LOCATION="gs://ttpos-dataflow-templates/staging/"

echo "ğŸ¯ é¡¹ç›®ID: $PROJECT_ID"
echo "ğŸŒ åŒºåŸŸ: $REGION"
echo "ğŸ“ ä½œä¸šåç§°: $JOB_NAME"
echo "ğŸ“ ä¸´æ—¶ä½ç½®: $TEMP_LOCATION"
echo "ğŸ“¦ æš‚å­˜ä½ç½®: $STAGING_LOCATION"

# åˆ›å»º Dataflow å…¼å®¹ç‰ˆæœ¬
cat > dataflow_runner.py << 'EOF'
#!/usr/bin/env python3
"""
Dataflow äº‘ç«¯è¿è¡Œç‰ˆæœ¬ - MySQL åˆ° BigQuery æ•°æ®åŒæ­¥
åŸºäº simple_sync_fixed.py çš„æˆåŠŸå®ç°
"""

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import mysql.connector
from google.cloud import bigquery
import json
import sys
from datetime import datetime
from decimal import Decimal

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64", "bigint": "INT64", "tinyint": "INT64", 
    "smallint": "INT64", "mediumint": "INT64",
    "decimal": "NUMERIC", "numeric": "NUMERIC",
    "float": "FLOAT64", "double": "FLOAT64",
    "varchar": "STRING", "char": "STRING", "text": "STRING",
    "mediumtext": "STRING", "longtext": "STRING",
    "date": "DATE", "datetime": "TIMESTAMP", "timestamp": "TIMESTAMP",
    "time": "STRING", "json": "STRING", "blob": "BYTES",
    "binary": "BYTES", "varbinary": "BYTES",
    "enum": "STRING", "set": "STRING"
}

class MySQLReader(beam.DoFn):
    def __init__(self, db_config, db_name, table_name):
        self.db_config = db_config
        self.db_name = db_name
        self.table_name = table_name
    
    def process(self, element):
        conn = mysql.connector.connect(
            host=self.db_config['db_host'],
            port=int(self.db_config['db_port']),
            user=self.db_config['db_user'],
            password=self.db_config['db_pass'],
            database=self.db_name
        )
        cursor = conn.cursor(dictionary=True)
        cursor.execute(f"SELECT * FROM {self.table_name}")
        
        for row in cursor.fetchall():
            # æ·»åŠ  tenant_id
            row['tenant_id'] = self.db_name
            
            # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
            for key, value in row.items():
                if isinstance(value, datetime):
                    row[key] = value.isoformat()
                elif isinstance(value, Decimal):
                    row[key] = float(value)
                elif value is None:
                    row[key] = None
            
            yield row
        
        cursor.close()
        conn.close()

def get_table_schema(db_config, db_name, table_name):
    """è·å–è¡¨ç»“æ„"""
    conn = mysql.connector.connect(
        host=db_config['db_host'],
        port=int(db_config['db_port']),
        user=db_config['db_user'],
        password=db_config['db_pass'],
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append({"name": field, "type": bq_type, "mode": "NULLABLE"})
    
    # æ·»åŠ  tenant_id å­—æ®µ
    schema.append({"name": "tenant_id", "type": "STRING", "mode": "NULLABLE"})
    
    cursor.close()
    conn.close()
    return schema

def run_pipeline():
    # è¯»å–é…ç½®
    with open('params.json', 'r') as f:
        params = json.load(f)
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    # Pipeline é€‰é¡¹
    pipeline_options = PipelineOptions([
        f'--project={params["bq_project"]}',
        f'--region={sys.argv[1] if len(sys.argv) > 1 else "us-central1"}',
        f'--job_name={sys.argv[2] if len(sys.argv) > 2 else "mysql-to-bq-sync"}',
        f'--temp_location={sys.argv[3] if len(sys.argv) > 3 else "gs://ttpos-dataflow-templates/tmp/"}',
        f'--staging_location={sys.argv[4] if len(sys.argv) > 4 else "gs://ttpos-dataflow-templates/staging/"}',
        '--runner=DataflowRunner',
        '--save_main_session=True'
    ])
    
    with beam.Pipeline(options=pipeline_options) as pipeline:
        for db_name in db_names:
            for table_name in table_names:
                # è·å–è¡¨ç»“æ„
                schema = get_table_schema(params, db_name, table_name)
                
                # åˆ›å»º BigQuery è¡¨è§„èŒƒ
                table_spec = f"{params['bq_project']}:{params['bq_dataset']}.{table_name}"
                
                # æ•°æ®å¤„ç†ç®¡é“
                (
                    pipeline
                    | f"Create-{db_name}-{table_name}" >> beam.Create([None])
                    | f"Read-{db_name}-{table_name}" >> beam.ParDo(MySQLReader(params, db_name, table_name))
                    | f"Write-{db_name}-{table_name}" >> beam.io.WriteToBigQuery(
                        table_spec,
                        schema=schema,
                        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,
                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
                    )
                )

if __name__ == "__main__":
    run_pipeline()
EOF

echo ""
echo "ğŸš€ å¯åŠ¨ Dataflow ä½œä¸š..."

# è¿è¡Œ Dataflow ä½œä¸š
python3 dataflow_runner.py \
    "$REGION" \
    "$JOB_NAME" \
    "$TEMP_LOCATION" \
    "$STAGING_LOCATION"

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… Dataflow ä½œä¸šå·²æˆåŠŸæäº¤!"
    echo "ğŸ”— æŸ¥çœ‹ä½œä¸šçŠ¶æ€: https://console.cloud.google.com/dataflow/jobs?project=$PROJECT_ID"
    echo "ğŸ“Š æŸ¥çœ‹ BigQuery æ•°æ®: https://console.cloud.google.com/bigquery?project=$PROJECT_ID"
else
    echo ""
    echo "âŒ Dataflow ä½œä¸šæäº¤å¤±è´¥"
    echo "ğŸ’¡ å»ºè®®ä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬: python3 simple_sync_fixed.py"
fi

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f dataflow_runner.py

echo ""
echo "ğŸ¯ ä½œä¸šä¿¡æ¯:"
echo "   é¡¹ç›®: $PROJECT_ID"
echo "   ä½œä¸šå: $JOB_NAME"
echo "   åŒºåŸŸ: $REGION"