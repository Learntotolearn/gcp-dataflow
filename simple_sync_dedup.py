#!/usr/bin/env python3
"""
MySQL åˆ° BigQuery æ•°æ®åŒæ­¥å·¥å…· - æ”¯æŒå»é‡çš„APPENDæ¨¡å¼
è§£å†³APPENDæ¨¡å¼ä¸‹çš„æ•°æ®é‡å¤é—®é¢˜
"""

import mysql.connector
from google.cloud import bigquery
import json
import sys
from datetime import datetime
from decimal import Decimal
import hashlib

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def get_table_primary_keys(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å–è¡¨çš„ä¸»é”®å­—æ®µ"""
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    
    # æŸ¥è¯¢ä¸»é”®ä¿¡æ¯
    cursor.execute(f"""
        SELECT COLUMN_NAME 
        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE 
        WHERE TABLE_SCHEMA = '{db_name}' 
        AND TABLE_NAME = '{table_name}' 
        AND CONSTRAINT_NAME = 'PRIMARY'
        ORDER BY ORDINAL_POSITION
    """)
    
    primary_keys = [row[0] for row in cursor.fetchall()]
    cursor.close()
    conn.close()
    
    print(f"  ğŸ”‘ ä¸»é”®å­—æ®µ: {primary_keys if primary_keys else 'æ— ä¸»é”®'}")
    return primary_keys

def get_table_schema(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ  tenant_id å­—æ®µ
    schema.append(bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"))
    # å¢åŠ åŒæ­¥æ—¶é—´æˆ³å­—æ®µ
    schema.append(bigquery.SchemaField("sync_timestamp", "TIMESTAMP", mode="NULLABLE"))
    # å¢åŠ æ•°æ®å“ˆå¸Œå­—æ®µï¼ˆç”¨äºå»é‡ï¼‰
    schema.append(bigquery.SchemaField("data_hash", "STRING", mode="NULLABLE"))
    
    cursor.close()
    conn.close()
    return schema

def generate_data_hash(row_data, exclude_fields=None):
    """ç”Ÿæˆæ•°æ®è¡Œçš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡"""
    if exclude_fields is None:
        exclude_fields = ['sync_timestamp', 'data_hash']
    
    # åˆ›å»ºç”¨äºå“ˆå¸Œçš„æ•°æ®å‰¯æœ¬
    hash_data = {}
    for key, value in row_data.items():
        if key not in exclude_fields:
            # ç»Ÿä¸€å¤„ç†æ•°æ®ç±»å‹
            if isinstance(value, datetime):
                hash_data[key] = value.isoformat()
            elif isinstance(value, Decimal):
                hash_data[key] = str(value)
            elif value is None:
                hash_data[key] = "NULL"
            else:
                hash_data[key] = str(value)
    
    # æŒ‰é”®æ’åºç¡®ä¿ä¸€è‡´æ€§
    sorted_data = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(sorted_data.encode('utf-8')).hexdigest()

def get_table_data(db_host, db_port, db_user, db_pass, db_name, table_name, incremental_field=None, last_sync_time=None):
    """è·å– MySQL è¡¨æ•°æ®ï¼Œæ”¯æŒå¢é‡åŒæ­¥"""
    print(f"ğŸ“¥ è¯»å–æ•°æ®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor(dictionary=True)
    
    # æ„å»ºæŸ¥è¯¢è¯­å¥
    if incremental_field and last_sync_time:
        query = f"SELECT * FROM {table_name} WHERE {incremental_field} > %s"
        cursor.execute(query, (last_sync_time,))
        print(f"  ğŸ”„ å¢é‡åŒæ­¥: {incremental_field} > {last_sync_time}")
    else:
        cursor.execute(f"SELECT * FROM {table_name}")
        print(f"  ğŸ“Š å…¨é‡åŒæ­¥")
    
    rows = []
    current_time = datetime.now()
    
    for row in cursor.fetchall():
        # æ·»åŠ  tenant_id
        row['tenant_id'] = db_name
        # æ·»åŠ åŒæ­¥æ—¶é—´æˆ³
        row['sync_timestamp'] = current_time
        
        # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.isoformat()
            elif isinstance(value, Decimal):
                row[key] = float(value)
            elif value is None:
                row[key] = None
        
        # ç”Ÿæˆæ•°æ®å“ˆå¸Œ
        row['data_hash'] = generate_data_hash(row)
        rows.append(row)
    
    print(f"  ğŸ“Š è¯»å–åˆ° {len(rows)} è¡Œæ•°æ®")
    cursor.close()
    conn.close()
    return rows

def get_last_sync_time(client, table_id):
    """è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´"""
    try:
        query = f"""
        SELECT MAX(sync_timestamp) as last_sync
        FROM `{table_id}`
        """
        result = client.query(query).result()
        for row in result:
            return row.last_sync
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´: {e}")
        return None

def sync_table_with_merge(params, db_name, table_name, primary_keys):
    """ä½¿ç”¨MERGEè¯­å¥åŒæ­¥è¡¨æ•°æ®ï¼Œå®ç°upsertæ“ä½œ"""
    print(f"\nğŸš€ å¼€å§‹MERGEåŒæ­¥: {db_name}.{table_name}")
    
    # è·å–è¡¨ç»“æ„
    schema = get_table_schema(
        params['db_host'], params['db_port'], 
        params['db_user'], params['db_pass'], 
        db_name, table_name
    )
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
    dataset_id = params['bq_dataset']
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    
    try:
        client.get_dataset(dataset_id)
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    try:
        table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
    except:
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
    
    # è·å–å¢é‡æ•°æ®
    incremental_field = params.get('incremental_field')
    last_sync_time = None
    
    if incremental_field:
        last_sync_time = get_last_sync_time(client, table_id)
        print(f"  ğŸ• ä¸Šæ¬¡åŒæ­¥æ—¶é—´: {last_sync_time}")
    
    # è·å–æ•°æ®
    rows = get_table_data(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name, incremental_field, last_sync_time
    )
    
    if not rows:
        print(f"  âš ï¸ æ— æ–°æ•°æ®ï¼Œè·³è¿‡åŒæ­¥")
        return
    
    # åˆ›å»ºä¸´æ—¶è¡¨
    temp_table_id = f"{table_id}_temp_{int(datetime.now().timestamp())}"
    temp_table = bigquery.Table(temp_table_id, schema=schema)
    temp_table = client.create_table(temp_table)
    print(f"  ğŸ”„ åˆ›å»ºä¸´æ—¶è¡¨: {temp_table_id}")
    
    try:
        # å°†æ•°æ®åŠ è½½åˆ°ä¸´æ—¶è¡¨
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
            schema=schema
        )
        
        job = client.load_table_from_json(rows, temp_table_id, job_config=job_config)
        job.result()
        print(f"  ğŸ“¥ æ•°æ®åŠ è½½åˆ°ä¸´æ—¶è¡¨: {len(rows)} è¡Œ")
        
        # æ„å»ºMERGEè¯­å¥
        if primary_keys:
            # æœ‰ä¸»é”®çš„æƒ…å†µï¼Œä½¿ç”¨ä¸»é”®è¿›è¡ŒMERGE
            merge_condition = " AND ".join([f"target.{pk} = source.{pk}" for pk in primary_keys])
        else:
            # æ— ä¸»é”®çš„æƒ…å†µï¼Œä½¿ç”¨æ•°æ®å“ˆå¸Œè¿›è¡ŒMERGE
            merge_condition = "target.data_hash = source.data_hash AND target.tenant_id = source.tenant_id"
        
        # è·å–æ‰€æœ‰å­—æ®µï¼ˆé™¤äº†ç”¨äºåŒ¹é…çš„å­—æ®µï¼‰
        all_fields = [field.name for field in schema if field.name not in ['data_hash']]
        update_fields = ", ".join([f"{field} = source.{field}" for field in all_fields])
        insert_fields = ", ".join(all_fields + ['data_hash'])
        insert_values = ", ".join([f"source.{field}" for field in all_fields + ['data_hash']])
        
        merge_query = f"""
        MERGE `{table_id}` AS target
        USING `{temp_table_id}` AS source
        ON {merge_condition}
        WHEN MATCHED THEN
          UPDATE SET {update_fields}
        WHEN NOT MATCHED THEN
          INSERT ({insert_fields})
          VALUES ({insert_values})
        """
        
        print(f"  ğŸ”„ æ‰§è¡ŒMERGEæ“ä½œ...")
        merge_job = client.query(merge_query)
        merge_result = merge_job.result()
        
        # è·å–MERGEç»Ÿè®¡ä¿¡æ¯
        print(f"  âœ… MERGEå®Œæˆ")
        
    finally:
        # æ¸…ç†ä¸´æ—¶è¡¨
        client.delete_table(temp_table_id)
        print(f"  ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶è¡¨")

def sync_table_with_dedup(params, db_name, table_name):
    """ä½¿ç”¨å»é‡é€»è¾‘åŒæ­¥è¡¨æ•°æ®"""
    print(f"\nğŸš€ å¼€å§‹å»é‡åŒæ­¥: {db_name}.{table_name}")
    
    # è·å–ä¸»é”®ä¿¡æ¯
    primary_keys = get_table_primary_keys(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name
    )
    
    # æ ¹æ®æ˜¯å¦æœ‰ä¸»é”®é€‰æ‹©ä¸åŒçš„åŒæ­¥ç­–ç•¥
    if params.get('dedup_mode', 'merge').lower() == 'merge':
        sync_table_with_merge(params, db_name, table_name, primary_keys)
    else:
        # ä¼ ç»Ÿçš„APPENDæ¨¡å¼ï¼Œä½†æ·»åŠ å»é‡é€»è¾‘
        sync_table_traditional_dedup(params, db_name, table_name)

def sync_table_traditional_dedup(params, db_name, table_name):
    """ä¼ ç»ŸAPPENDæ¨¡å¼ + å»é‡é€»è¾‘"""
    print(f"  ğŸ“ ä½¿ç”¨ä¼ ç»Ÿå»é‡æ¨¡å¼")
    
    # è·å–è¡¨ç»“æ„
    schema = get_table_schema(
        params['db_host'], params['db_port'], 
        params['db_user'], params['db_pass'], 
        db_name, table_name
    )
    
    # è·å–è¡¨æ•°æ®
    rows = get_table_data(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name
    )
    
    if not rows:
        print(f"  âš ï¸ è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®åŒæ­¥")
        return
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
    dataset_id = params['bq_dataset']
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    
    try:
        client.get_dataset(dataset_id)
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    try:
        table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
    except:
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
    
    # æŸ¥è¯¢ç°æœ‰æ•°æ®çš„å“ˆå¸Œå€¼
    existing_hashes = set()
    try:
        query = f"SELECT DISTINCT data_hash FROM `{table_id}` WHERE tenant_id = '{db_name}'"
        result = client.query(query).result()
        existing_hashes = {row.data_hash for row in result}
        print(f"  ğŸ” ç°æœ‰æ•°æ®å“ˆå¸Œ: {len(existing_hashes)} ä¸ª")
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•æŸ¥è¯¢ç°æœ‰æ•°æ®: {e}")
    
    # è¿‡æ»¤é‡å¤æ•°æ®
    new_rows = []
    duplicate_count = 0
    
    for row in rows:
        if row['data_hash'] not in existing_hashes:
            new_rows.append(row)
        else:
            duplicate_count += 1
    
    print(f"  ğŸ“Š æ–°æ•°æ®: {len(new_rows)} è¡Œ, é‡å¤æ•°æ®: {duplicate_count} è¡Œ")
    
    if new_rows:
        # æ’å…¥æ–°æ•°æ®
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            schema=schema
        )
        
        job = client.load_table_from_json(new_rows, table_id, job_config=job_config)
        job.result()
        
        print(f"  âœ… æ–°æ•°æ®åŒæ­¥å®Œæˆ: {len(new_rows)} è¡Œ")
    else:
        print(f"  âš ï¸ æ— æ–°æ•°æ®éœ€è¦åŒæ­¥")

def main():
    print("ğŸš€ MySQL åˆ° BigQuery å»é‡åŒæ­¥å·¥å…·")
    print("=" * 50)
    
    # è¯»å–é…ç½®
    with open('params.json', 'r') as f:
        params = json.load(f)
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸ“Š ç›®æ ‡: {params['bq_project']}.{params['bq_dataset']}")
    print(f"ğŸ”§ å»é‡æ¨¡å¼: {params.get('dedup_mode', 'merge')}")
    
    # åŒæ­¥æ¯ä¸ªè¡¨
    total_tables = len(db_names) * len(table_names)
    current = 0
    success_count = 0
    
    for db_name in db_names:
        for table_name in table_names:
            current += 1
            print(f"\n[{current}/{total_tables}] å¤„ç†ä¸­...")
            try:
                sync_table_with_dedup(params, db_name, table_name)
                success_count += 1
            except Exception as e:
                print(f"âŒ åŒæ­¥å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
    
    print(f"\nğŸ‰ å»é‡åŒæ­¥å®Œæˆï¼æˆåŠŸå¤„ç†äº† {success_count}/{total_tables} ä¸ªè¡¨")

if __name__ == "__main__":
    main()