#!/usr/bin/env python3
"""
MySQL åˆ° BigQuery å¢é‡åŒæ­¥å·¥å…· - å…¼å®¹ç‰ˆæœ¬
å…¼å®¹ç°æœ‰è¡¨ç»“æ„ï¼Œä¸å¼ºåˆ¶æ·»åŠ sync_timestampå­—æ®µ
"""

import mysql.connector
from google.cloud import bigquery
import json
import sys
from datetime import datetime, timedelta
from decimal import Decimal

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def detect_timestamp_fields(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è‡ªåŠ¨æ£€æµ‹è¡¨ä¸­çš„æ—¶é—´æˆ³å­—æ®µ"""
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    
    # æŸ¥è¯¢è¡¨ç»“æ„ï¼Œå¯»æ‰¾æ—¶é—´æˆ³å­—æ®µ
    cursor.execute(f"""
        SELECT COLUMN_NAME, DATA_TYPE, COLUMN_DEFAULT
        FROM INFORMATION_SCHEMA.COLUMNS 
        WHERE TABLE_SCHEMA = '{db_name}' 
        AND TABLE_NAME = '{table_name}'
        AND DATA_TYPE IN ('timestamp', 'datetime', 'int', 'bigint')
        ORDER BY ORDINAL_POSITION
    """)
    
    timestamp_fields = []
    for column_name, data_type, column_default in cursor.fetchall():
        # å¸¸è§çš„æ—¶é—´æˆ³å­—æ®µå
        if any(keyword in column_name.lower() for keyword in 
               ['time', 'date', 'created', 'updated', 'modified']):
            timestamp_fields.append({
                'name': column_name,
                'type': data_type,
                'default': column_default
            })
    
    cursor.close()
    conn.close()
    
    print(f"  ğŸ• æ£€æµ‹åˆ°æ—¶é—´æˆ³å­—æ®µ: {[f['name'] for f in timestamp_fields]}")
    return timestamp_fields

def get_table_schema_compatible(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema - å…¼å®¹ç‰ˆæœ¬"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ  tenant_id å­—æ®µ
    schema.append(bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"))
    
    cursor.close()
    conn.close()
    return schema

def get_last_sync_timestamp_compatible(client, table_id, timestamp_field, tenant_id):
    """è·å–æŒ‡å®šç§Ÿæˆ·çš„æœ€ååŒæ­¥æ—¶é—´æˆ³ - å…¼å®¹ç‰ˆæœ¬"""
    try:
        # æ ¹æ®æ—¶é—´æˆ³å­—æ®µç±»å‹æ„å»ºæŸ¥è¯¢
        if timestamp_field['type'] in ['timestamp', 'datetime']:
            query = f"""
            SELECT MAX(UNIX_TIMESTAMP({timestamp_field['name']})) as last_timestamp
            FROM `{table_id}`
            WHERE tenant_id = '{tenant_id}'
            """
        else:  # int, bigintç±»å‹çš„æ—¶é—´æˆ³
            query = f"""
            SELECT MAX({timestamp_field['name']}) as last_timestamp
            FROM `{table_id}`
            WHERE tenant_id = '{tenant_id}'
            """
        
        result = client.query(query).result()
        for row in result:
            if row.last_timestamp:
                return int(row.last_timestamp)
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´æˆ³: {e}")
    
    return None

def get_incremental_data_compatible(db_host, db_port, db_user, db_pass, db_name, table_name, 
                                  timestamp_field, last_timestamp=None, lookback_hours=1):
    """è·å–å¢é‡æ•°æ® - å…¼å®¹ç‰ˆæœ¬"""
    print(f"ğŸ“¥ è¯»å–å¢é‡æ•°æ®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor(dictionary=True)
    
    # æ„å»ºå¢é‡æŸ¥è¯¢æ¡ä»¶
    if last_timestamp:
        # å‡å»å›çœ‹æ—¶é—´ï¼Œé˜²æ­¢é—æ¼æ•°æ®
        lookback_timestamp = last_timestamp - (lookback_hours * 3600)
        
        if timestamp_field['type'] in ['timestamp', 'datetime']:
            query = f"""
            SELECT * FROM {table_name} 
            WHERE UNIX_TIMESTAMP({timestamp_field['name']}) > %s
            ORDER BY {timestamp_field['name']}
            """
        else:  # int, bigintç±»å‹
            query = f"""
            SELECT * FROM {table_name} 
            WHERE {timestamp_field['name']} > %s
            ORDER BY {timestamp_field['name']}
            """
        
        cursor.execute(query, (lookback_timestamp,))
        print(f"  ğŸ”„ å¢é‡åŒæ­¥: {timestamp_field['name']} > {lookback_timestamp}")
    else:
        # é¦–æ¬¡åŒæ­¥ï¼Œè·å–æœ€è¿‘24å°æ—¶çš„æ•°æ®
        if timestamp_field['type'] in ['timestamp', 'datetime']:
            query = f"""
            SELECT * FROM {table_name} 
            WHERE {timestamp_field['name']} >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
            ORDER BY {timestamp_field['name']}
            """
            cursor.execute(query)
        else:  # intç±»å‹æ—¶é—´æˆ³
            current_timestamp = int(datetime.now().timestamp())
            day_ago_timestamp = current_timestamp - (24 * 3600)
            query = f"""
            SELECT * FROM {table_name} 
            WHERE {timestamp_field['name']} >= %s
            ORDER BY {timestamp_field['name']}
            """
            cursor.execute(query, (day_ago_timestamp,))
        
        print(f"  ğŸ†• é¦–æ¬¡åŒæ­¥: è·å–æœ€è¿‘24å°æ—¶æ•°æ®")
    
    rows = []
    for row in cursor.fetchall():
        # æ·»åŠ  tenant_id
        row['tenant_id'] = db_name
        
        # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.isoformat()
            elif isinstance(value, Decimal):
                row[key] = float(value)
            elif value is None:
                row[key] = None
        
        rows.append(row)
    
    print(f"  ğŸ“Š è¯»å–åˆ° {len(rows)} è¡Œå¢é‡æ•°æ®")
    cursor.close()
    conn.close()
    return rows

def sync_table_incremental_compatible(params, db_name, table_name):
    """å¢é‡åŒæ­¥å•ä¸ªè¡¨ - å…¼å®¹ç‰ˆæœ¬"""
    print(f"\nğŸš€ å¼€å§‹å¢é‡åŒæ­¥: {db_name}.{table_name}")
    
    # æ£€æµ‹æ—¶é—´æˆ³å­—æ®µ
    timestamp_fields = detect_timestamp_fields(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name
    )
    
    if not timestamp_fields:
        print(f"  âš ï¸ æœªæ£€æµ‹åˆ°æ—¶é—´æˆ³å­—æ®µï¼Œè·³è¿‡å¢é‡åŒæ­¥")
        return
    
    # é€‰æ‹©ä¸»è¦çš„æ—¶é—´æˆ³å­—æ®µï¼ˆä¼˜å…ˆé€‰æ‹©update_time, modified_timeç­‰ï¼‰
    primary_timestamp = None
    for field in timestamp_fields:
        if any(keyword in field['name'].lower() for keyword in ['update', 'modified']):
            primary_timestamp = field
            break
    
    if not primary_timestamp:
        primary_timestamp = timestamp_fields[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³å­—æ®µ
    
    print(f"  ğŸ¯ ä½¿ç”¨æ—¶é—´æˆ³å­—æ®µ: {primary_timestamp['name']} ({primary_timestamp['type']})")
    
    # è·å–è¡¨ç»“æ„
    schema = get_table_schema_compatible(
        params['db_host'], params['db_port'], 
        params['db_user'], params['db_pass'], 
        db_name, table_name
    )
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†å’Œè¡¨
    dataset_id = params['bq_dataset']
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    
    try:
        client.get_dataset(dataset_id)
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    try:
        target_table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
        # è·å–ç°æœ‰è¡¨çš„schema
        existing_schema = target_table.schema
        existing_field_names = [field.name for field in existing_schema]
    except:
        # è¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨
        table = bigquery.Table(table_id, schema=schema)
        target_table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
        existing_field_names = [field.name for field in schema]
    
    # è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´æˆ³
    last_timestamp = get_last_sync_timestamp_compatible(client, table_id, primary_timestamp, db_name)
    print(f"  ğŸ• ä¸Šæ¬¡åŒæ­¥æ—¶é—´æˆ³: {last_timestamp}")
    
    # è·å–å¢é‡æ•°æ®
    lookback_hours = params.get('lookback_hours', 1)
    rows = get_incremental_data_compatible(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name, primary_timestamp, 
        last_timestamp, lookback_hours
    )
    
    if not rows:
        print(f"  âš ï¸ æ— å¢é‡æ•°æ®ï¼Œè·³è¿‡åŒæ­¥")
        return
    
    # ä½¿ç”¨MERGEè¯­å¥è¿›è¡Œupsertæ“ä½œ
    temp_table_id = f"{table_id}_temp_{int(datetime.now().timestamp())}"
    
    # åˆ›å»ºä¸´æ—¶è¡¨schemaï¼ŒåªåŒ…å«ç°æœ‰è¡¨ä¸­å­˜åœ¨çš„å­—æ®µ
    temp_schema = []
    for field in schema:
        if field.name in existing_field_names:
            temp_schema.append(field)
    
    temp_table = bigquery.Table(temp_table_id, schema=temp_schema)
    temp_table = client.create_table(temp_table)
    print(f"  ğŸ”„ åˆ›å»ºä¸´æ—¶è¡¨: {temp_table_id}")
    
    try:
        # è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™ç°æœ‰è¡¨ä¸­å­˜åœ¨çš„å­—æ®µ
        filtered_rows = []
        for row in rows:
            filtered_row = {}
            for key, value in row.items():
                if key in existing_field_names:
                    filtered_row[key] = value
            filtered_rows.append(filtered_row)
        
        # å°†æ•°æ®åŠ è½½åˆ°ä¸´æ—¶è¡¨
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
            schema=temp_schema
        )
        
        job = client.load_table_from_json(filtered_rows, temp_table_id, job_config=job_config)
        job.result()
        print(f"  ğŸ“¥ æ•°æ®åŠ è½½åˆ°ä¸´æ—¶è¡¨: {len(filtered_rows)} è¡Œ")
        
        # æ„å»ºMERGEè¯­å¥ - åŸºäºæ—¶é—´æˆ³+tenant_id
        merge_condition = f"target.{primary_timestamp['name']} = source.{primary_timestamp['name']} AND target.tenant_id = source.tenant_id"
        
        # åªä½¿ç”¨ç°æœ‰è¡¨ä¸­å­˜åœ¨çš„å­—æ®µ
        available_fields = [field.name for field in temp_schema]
        update_fields = ", ".join([f"{field} = source.{field}" for field in available_fields])
        insert_fields = ", ".join(available_fields)
        insert_values = ", ".join([f"source.{field}" for field in available_fields])
        
        merge_query = f"""
        MERGE `{table_id}` AS target
        USING `{temp_table_id}` AS source
        ON {merge_condition}
        WHEN MATCHED THEN
          UPDATE SET {update_fields}
        WHEN NOT MATCHED THEN
          INSERT ({insert_fields})
          VALUES ({insert_values})
        """
        
        print(f"  ğŸ”„ æ‰§è¡ŒMERGEæ“ä½œ...")
        print(f"  ğŸ“‹ ä½¿ç”¨å­—æ®µ: {len(available_fields)} ä¸ª")
        
        merge_job = client.query(merge_query)
        merge_result = merge_job.result()
        
        print(f"  âœ… å¢é‡åŒæ­¥å®Œæˆ")
        
    finally:
        # æ¸…ç†ä¸´æ—¶è¡¨
        client.delete_table(temp_table_id)
        print(f"  ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶è¡¨")

def main():
    print("ğŸš€ MySQL åˆ° BigQuery å¢é‡åŒæ­¥å·¥å…· - å…¼å®¹ç‰ˆæœ¬")
    print("=" * 60)
    
    # è¯»å–é…ç½®
    with open('params.json', 'r') as f:
        params = json.load(f)
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸ“Š ç›®æ ‡: {params['bq_project']}.{params['bq_dataset']}")
    print(f"â° å›çœ‹æ—¶é—´: {params.get('lookback_hours', 1)} å°æ—¶")
    
    # åŒæ­¥æ¯ä¸ªè¡¨
    total_tables = len(db_names) * len(table_names)
    current = 0
    success_count = 0
    
    for db_name in db_names:
        for table_name in table_names:
            current += 1
            print(f"\n[{current}/{total_tables}] å¤„ç†ä¸­...")
            try:
                sync_table_incremental_compatible(params, db_name, table_name)
                success_count += 1
            except Exception as e:
                print(f"âŒ åŒæ­¥å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
    
    print(f"\nğŸ‰ å¢é‡åŒæ­¥å®Œæˆï¼æˆåŠŸå¤„ç†äº† {success_count}/{total_tables} ä¸ªè¡¨")

if __name__ == "__main__":
    main()