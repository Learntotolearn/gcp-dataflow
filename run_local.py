#!/usr/bin/env python3
"""
ç›´æ¥è¿è¡Œ MySQL åˆ° BigQuery æ•°æ®åŒæ­¥è„šæœ¬
æ— éœ€å®¹å™¨ï¼Œæœ¬åœ°ç›´æ¥æ‰§è¡Œ
"""

import argparse
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions
from apache_beam.io.jdbc import ReadFromJdbc
from apache_beam.io.gcp.bigquery import WriteToBigQuery
from apache_beam.io.gcp.internal.clients import bigquery
import mysql.connector
import json
import os

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64", 
    "tinyint": "INT64",
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC", 
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES", 
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def get_table_schema(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host, 
        port=int(db_port), 
        user=db_user, 
        password=db_pass, 
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = {"fields": []}
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema["fields"].append({
            "name": field, 
            "type": bq_type,
            "mode": "NULLABLE"
        })
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ  tenant_id å­—æ®µ
    schema["fields"].append({
        "name": "tenant_id", 
        "type": "STRING",
        "mode": "NULLABLE"
    })
    
    cursor.close()
    conn.close()
    return schema

def process_table(p, db_name, table_name, args):
    """å¤„ç†å•ä¸ªåº“çš„å•å¼ è¡¨"""
    print(f"ğŸš€ å¼€å§‹å¤„ç†: {db_name}.{table_name}")
    
    jdbc_url = f"jdbc:mysql://{args.db_host}:{args.db_port}/{db_name}?useSSL=false&allowPublicKeyRetrieval=true"
    
    schema = get_table_schema(
        args.db_host, args.db_port, args.db_user, args.db_pass, db_name, table_name
    )
    
    bq_table = f"{args.bq_project}:{args.bq_dataset}.{table_name}"
    print(f"ğŸ“Š ç›®æ ‡è¡¨: {bq_table}")
    
    (
        p
        | f"Read-{db_name}-{table_name}" >> ReadFromJdbc(
            table_name=table_name,
            driver_class_name="com.mysql.cj.jdbc.Driver", 
            jdbc_url=jdbc_url,
            username=args.db_user,
            password=args.db_pass,
        )
        | f"AddTenant-{db_name}-{table_name}" >> beam.Map(
            lambda row, tenant=db_name: {**row, "tenant_id": tenant}
        )
        | f"WriteBQ-{db_name}-{table_name}" >> WriteToBigQuery(
            table=bq_table,
            schema=schema,
            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
        )
    )

def run():
    parser = argparse.ArgumentParser(description="MySQL åˆ° BigQuery æ•°æ®åŒæ­¥å·¥å…·")
    
    # æ•°æ®åº“è¿æ¥å‚æ•°
    parser.add_argument("--db_host", required=True, help="MySQL ä¸»æœºåœ°å€")
    parser.add_argument("--db_port", default="3306", help="MySQL ç«¯å£")
    parser.add_argument("--db_user", required=True, help="MySQL ç”¨æˆ·å")
    parser.add_argument("--db_pass", required=True, help="MySQL å¯†ç ")
    parser.add_argument("--db_list", required=True, help="æ•°æ®åº“åˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--table_list", required=True, help="è¡¨ååˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    
    # BigQuery å‚æ•°
    parser.add_argument("--bq_project", required=True, help="BigQuery é¡¹ç›®ID")
    parser.add_argument("--bq_dataset", required=True, help="BigQuery æ•°æ®é›†")
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--runner", default="DirectRunner", 
                       choices=["DirectRunner", "DataflowRunner"],
                       help="è¿è¡Œå™¨ç±»å‹")
    parser.add_argument("--region", default="us-central1", help="Dataflow åŒºåŸŸ")
    parser.add_argument("--temp_location", help="ä¸´æ—¶æ–‡ä»¶ä½ç½® (GCS)")
    parser.add_argument("--staging_location", help="æš‚å­˜æ–‡ä»¶ä½ç½® (GCS)")
    
    args = parser.parse_args()
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in args.db_list.split(",")]
    table_names = [t.strip() for t in args.table_list.split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸƒ è¿è¡Œå™¨: {args.runner}")
    
    # è®¾ç½® Pipeline é€‰é¡¹
    pipeline_options = PipelineOptions()
    
    if args.runner == "DataflowRunner":
        # Dataflow è¿è¡Œå™¨é…ç½®
        google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
        google_cloud_options.project = args.bq_project
        google_cloud_options.region = args.region
        google_cloud_options.job_name = f"mysql-to-bq-{int(time.time())}"
        
        if args.temp_location:
            google_cloud_options.temp_location = args.temp_location
        if args.staging_location:
            google_cloud_options.staging_location = args.staging_location
            
        pipeline_options.view_as(StandardOptions).runner = "DataflowRunner"
    else:
        # æœ¬åœ°è¿è¡Œå™¨é…ç½®
        pipeline_options.view_as(StandardOptions).runner = "DirectRunner"
    
    pipeline_options.view_as(StandardOptions).streaming = False
    
    # è¿è¡Œ Pipeline
    with beam.Pipeline(options=pipeline_options) as p:
        for db_name in db_names:
            for table_name in table_names:
                process_table(p, db_name, table_name, args)
    
    print("âœ… æ•°æ®åŒæ­¥å®Œæˆ!")

if __name__ == "__main__":
    import time
    run()