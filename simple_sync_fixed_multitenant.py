#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ MySQL åˆ° BigQuery æ•°æ®åŒæ­¥å·¥å…· - å¤šç§Ÿæˆ·ä¿®å¤ç‰ˆ
ä¿®å¤å¤šç§Ÿæˆ·åœºæ™¯ä¸‹TRUNCATEæ¨¡å¼å¯¼è‡´æ•°æ®è¦†ç›–çš„é—®é¢˜
"""

import mysql.connector
from google.cloud import bigquery
import json
import sys
import hashlib
from datetime import datetime
from decimal import Decimal

# MySQL -> BigQuery ç±»å‹æ˜ å°„
MYSQL_TO_BQ_TYPE = {
    "int": "INT64",
    "bigint": "INT64",
    "tinyint": "INT64", 
    "smallint": "INT64",
    "mediumint": "INT64",
    "decimal": "NUMERIC",
    "numeric": "NUMERIC",
    "float": "FLOAT64",
    "double": "FLOAT64",
    "varchar": "STRING",
    "char": "STRING",
    "text": "STRING",
    "mediumtext": "STRING",
    "longtext": "STRING",
    "date": "DATE",
    "datetime": "TIMESTAMP",
    "timestamp": "TIMESTAMP",
    "time": "STRING",
    "json": "STRING",
    "blob": "BYTES",
    "binary": "BYTES",
    "varbinary": "BYTES",
    "enum": "STRING",
    "set": "STRING"
}

def generate_data_hash(row_data):
    """ç”Ÿæˆæ•°æ®è¡Œçš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ™ºèƒ½å»é‡"""
    # æ’é™¤ç³»ç»Ÿå­—æ®µï¼šå“ˆå¸Œå­—æ®µã€ç§Ÿæˆ·å­—æ®µã€æ—¶é—´æˆ³å­—æ®µ
    hash_data = {}
    for key, value in row_data.items():
        if key not in ['data_hash', 'tenant_id', 'last_updated']:
            if isinstance(value, datetime):
                hash_data[key] = value.isoformat()
            elif isinstance(value, Decimal):
                hash_data[key] = str(value)
            elif value is None:
                hash_data[key] = "NULL"
            else:
                hash_data[key] = value
    
    # ç”Ÿæˆå“ˆå¸Œ
    row_str = json.dumps(hash_data, sort_keys=True, default=str, ensure_ascii=False)
    return hashlib.md5(row_str.encode('utf-8')).hexdigest()

def get_table_schema(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º BigQuery schema"""
    print(f"ğŸ” è·å–è¡¨ç»“æ„: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor()
    cursor.execute(f"DESCRIBE {table_name}")
    
    schema = []
    for field, ftype, *_ in cursor.fetchall():
        base_type = ftype.split("(")[0].lower()
        bq_type = MYSQL_TO_BQ_TYPE.get(base_type, "STRING")
        schema.append(bigquery.SchemaField(field, bq_type, mode="NULLABLE"))
        print(f"  ğŸ“‹ {field}: {ftype} -> {bq_type}")
    
    # å¢åŠ å¤šç§Ÿæˆ·å­—æ®µ
    schema.append(bigquery.SchemaField("tenant_id", "STRING", mode="NULLABLE"))
    schema.append(bigquery.SchemaField("data_hash", "STRING", mode="NULLABLE"))
    schema.append(bigquery.SchemaField("last_updated", "TIMESTAMP", mode="NULLABLE"))
    
    cursor.close()
    conn.close()
    return schema

def get_table_data(db_host, db_port, db_user, db_pass, db_name, table_name):
    """è·å– MySQL è¡¨æ•°æ®"""
    print(f"ğŸ“¥ è¯»å–æ•°æ®: {db_name}.{table_name}")
    
    conn = mysql.connector.connect(
        host=db_host,
        port=int(db_port),
        user=db_user,
        password=db_pass,
        database=db_name
    )
    cursor = conn.cursor(dictionary=True)
    cursor.execute(f"SELECT * FROM {table_name}")
    
    rows = []
    current_time = datetime.now().isoformat()
    
    for row in cursor.fetchall():
        # æ·»åŠ  tenant_id
        row['tenant_id'] = db_name
        
        # æ·»åŠ åŒæ­¥æ—¶é—´æˆ³
        row['last_updated'] = current_time
        
        # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
        for key, value in row.items():
            if isinstance(value, datetime):
                row[key] = value.isoformat()
            elif isinstance(value, Decimal):
                row[key] = float(value)
            elif value is None:
                row[key] = None
        
        # ç”Ÿæˆæ™ºèƒ½å“ˆå¸Œç”¨äºå»é‡
        row['data_hash'] = generate_data_hash(row)
        
        rows.append(row)
    
    print(f"  ğŸ“Š è¯»å–åˆ° {len(rows)} è¡Œæ•°æ®")
    cursor.close()
    conn.close()
    return rows

def sync_table_multitenant(params, table_name, db_names):
    """å¤šç§Ÿæˆ·åŒæ­¥å•ä¸ªè¡¨ - ä¿®å¤ç‰ˆæœ¬"""
    print(f"\nğŸš€ å¼€å§‹å¤šç§Ÿæˆ·åŒæ­¥è¡¨: {table_name}")
    print(f"ğŸ¢ æ¶‰åŠç§Ÿæˆ·: {db_names}")
    
    # æ”¶é›†æ‰€æœ‰ç§Ÿæˆ·çš„æ•°æ®
    all_rows = []
    schema = None
    
    for db_name in db_names:
        try:
            # è·å–è¡¨ç»“æ„ï¼ˆåªéœ€è¦è·å–ä¸€æ¬¡ï¼Œæ‰€æœ‰ç§Ÿæˆ·ç»“æ„ç›¸åŒï¼‰
            if schema is None:
                schema = get_table_schema(
                    params['db_host'], params['db_port'], 
                    params['db_user'], params['db_pass'], 
                    db_name, table_name
                )
            
            # è·å–è¯¥ç§Ÿæˆ·çš„è¡¨æ•°æ®
            rows = get_table_data(
                params['db_host'], params['db_port'],
                params['db_user'], params['db_pass'],
                db_name, table_name
            )
            
            # æ·»åŠ åˆ°æ€»æ•°æ®é›†
            all_rows.extend(rows)
            print(f"  âœ… ç§Ÿæˆ· {db_name}: {len(rows)} è¡Œæ•°æ®")
            
        except Exception as e:
            print(f"  âŒ ç§Ÿæˆ· {db_name} åŒæ­¥å¤±è´¥: {str(e)}")
            continue
    
    if not all_rows:
        print(f"  âš ï¸ è¡¨ {table_name} æ— æ•°æ®ï¼Œè·³è¿‡åŒæ­¥")
        return
    
    print(f"ğŸ“Š æ€»è®¡æ”¶é›†æ•°æ®: {len(all_rows)} è¡Œ")
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    dataset_id = params['bq_dataset']
    try:
        client.get_dataset(dataset_id)
        print(f"  âœ… æ•°æ®é›†å·²å­˜åœ¨: {dataset_id}")
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    try:
        table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
    except:
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
    
    # ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰ç§Ÿæˆ·æ•°æ®
    write_mode = params.get('write_mode', 'TRUNCATE').upper()
    if write_mode == 'APPEND':
        write_disposition = bigquery.WriteDisposition.WRITE_APPEND
        print(f"  ğŸ“ å†™å…¥æ¨¡å¼: è¿½åŠ  (APPEND)")
    elif write_mode == 'EMPTY':
        write_disposition = bigquery.WriteDisposition.WRITE_EMPTY
        print(f"  ğŸ“ å†™å…¥æ¨¡å¼: ä»…ç©ºè¡¨ (EMPTY)")
    else:  # é»˜è®¤ TRUNCATE
        write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
        print(f"  ğŸ“ å†™å…¥æ¨¡å¼: è¦†ç›– (TRUNCATE)")
    
    job_config = bigquery.LoadJobConfig(
        write_disposition=write_disposition,
        schema=schema
    )
    
    job = client.load_table_from_json(all_rows, table_id, job_config=job_config)
    job.result()  # ç­‰å¾…ä½œä¸šå®Œæˆ
    
    print(f"  âœ… è¡¨ {table_name} å¤šç§Ÿæˆ·æ•°æ®åŒæ­¥å®Œæˆ: {len(all_rows)} è¡Œ")
    
    # æ˜¾ç¤ºå„ç§Ÿæˆ·æ•°æ®ç»Ÿè®¡
    tenant_stats = {}
    for row in all_rows:
        tenant_id = row['tenant_id']
        tenant_stats[tenant_id] = tenant_stats.get(tenant_id, 0) + 1
    
    print(f"  ğŸ“ˆ å„ç§Ÿæˆ·æ•°æ®ç»Ÿè®¡:")
    for tenant_id, count in tenant_stats.items():
        print(f"    ğŸ¢ {tenant_id}: {count} è¡Œ")

def sync_single_table_single_tenant(params, db_name, table_name, write_mode_override=None):
    """å•ç§Ÿæˆ·å•è¡¨åŒæ­¥ - å…¼å®¹åŸæœ‰é€»è¾‘"""
    print(f"\nğŸš€ å¼€å§‹å•ç§Ÿæˆ·åŒæ­¥: {db_name}.{table_name}")
    
    # è·å–è¡¨ç»“æ„
    schema = get_table_schema(
        params['db_host'], params['db_port'], 
        params['db_user'], params['db_pass'], 
        db_name, table_name
    )
    
    # è·å–è¡¨æ•°æ®
    rows = get_table_data(
        params['db_host'], params['db_port'],
        params['db_user'], params['db_pass'],
        db_name, table_name
    )
    
    # åˆ›å»º BigQuery å®¢æˆ·ç«¯
    client = bigquery.Client(project=params['bq_project'])
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    dataset_id = params['bq_dataset']
    try:
        client.get_dataset(dataset_id)
        print(f"  âœ… æ•°æ®é›†å·²å­˜åœ¨: {dataset_id}")
    except:
        dataset = bigquery.Dataset(f"{params['bq_project']}.{dataset_id}")
        dataset.location = "US"
        client.create_dataset(dataset)
        print(f"  ğŸ†• åˆ›å»ºæ•°æ®é›†: {dataset_id}")
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    table_id = f"{params['bq_project']}.{dataset_id}.{table_name}"
    try:
        table = client.get_table(table_id)
        print(f"  âœ… è¡¨å·²å­˜åœ¨: {table_name}")
    except:
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        print(f"  ğŸ†• åˆ›å»ºè¡¨: {table_name}")
    
    # æ’å…¥æ•°æ®
    if rows:
        # æ ¹æ®é…ç½®é€‰æ‹©å†™å…¥æ¨¡å¼
        write_mode = write_mode_override or params.get('write_mode', 'TRUNCATE').upper()
        if write_mode == 'APPEND':
            write_disposition = bigquery.WriteDisposition.WRITE_APPEND
            print(f"  ğŸ“ å†™å…¥æ¨¡å¼: è¿½åŠ  (APPEND)")
        elif write_mode == 'EMPTY':
            write_disposition = bigquery.WriteDisposition.WRITE_EMPTY
            print(f"  ğŸ“ å†™å…¥æ¨¡å¼: ä»…ç©ºè¡¨ (EMPTY)")
        else:  # é»˜è®¤ TRUNCATE
            write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
            print(f"  ğŸ“ å†™å…¥æ¨¡å¼: è¦†ç›– (TRUNCATE)")
        
        job_config = bigquery.LoadJobConfig(
            write_disposition=write_disposition,
            schema=schema
        )
        
        job = client.load_table_from_json(rows, table_id, job_config=job_config)
        job.result()  # ç­‰å¾…ä½œä¸šå®Œæˆ
        
        print(f"  âœ… æ•°æ®åŒæ­¥å®Œæˆ: {len(rows)} è¡Œ")
    else:
        print(f"  âš ï¸ è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®åŒæ­¥")

def main():
    print("ğŸš€ MySQL åˆ° BigQuery å¤šç§Ÿæˆ·ä¿®å¤åŒæ­¥å·¥å…·")
    print("=" * 60)
    
    # è¯»å–é…ç½®
    with open('params.json', 'r') as f:
        params = json.load(f)
    
    # è§£ææ•°æ®åº“å’Œè¡¨åˆ—è¡¨
    db_names = [db.strip() for db in params['db_list'].split(",")]
    table_names = [t.strip() for t in params['table_list'].split(",")]
    
    print(f"ğŸ¯ æ•°æ®åº“: {db_names}")
    print(f"ğŸ“‹ è¡¨å: {table_names}")
    print(f"ğŸ“Š ç›®æ ‡: {params['bq_project']}.{params['bq_dataset']}")
    print(f"ğŸ”§ å†™å…¥æ¨¡å¼: {params.get('write_mode', 'TRUNCATE')}")
    
    # æ£€æµ‹æ˜¯å¦ä¸ºå¤šç§Ÿæˆ·åœºæ™¯
    is_multitenant = len(db_names) > 1
    print(f"ğŸ¢ å¤šç§Ÿæˆ·æ¨¡å¼: {'æ˜¯' if is_multitenant else 'å¦'}")
    
    if is_multitenant:
        print("\nğŸ”„ ä½¿ç”¨å¤šç§Ÿæˆ·åŒæ­¥ç­–ç•¥ï¼ˆæŒ‰è¡¨åˆ†ç»„ï¼‰")
        # å¤šç§Ÿæˆ·åœºæ™¯ï¼šæŒ‰è¡¨åˆ†ç»„åŒæ­¥
        success_count = 0
        for i, table_name in enumerate(table_names, 1):
            print(f"\n[{i}/{len(table_names)}] å¤„ç†è¡¨: {table_name}")
            try:
                sync_table_multitenant(params, table_name, db_names)
                success_count += 1
            except Exception as e:
                print(f"âŒ è¡¨ {table_name} åŒæ­¥å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
        
        print(f"\nğŸ‰ å¤šç§Ÿæˆ·åŒæ­¥å®Œæˆï¼æˆåŠŸå¤„ç†äº† {success_count}/{len(table_names)} å¼ è¡¨")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š åŒæ­¥ç»Ÿè®¡:")
        print(f"  ğŸ¢ ç§Ÿæˆ·æ•°é‡: {len(db_names)}")
        print(f"  ğŸ“‹ è¡¨æ•°é‡: {len(table_names)}")
        print(f"  âœ… æˆåŠŸè¡¨æ•°: {success_count}")
        print(f"  ğŸ“ˆ æ€»åŒæ­¥ä»»åŠ¡: {success_count * len(db_names)} ä¸ªç§Ÿæˆ·è¡¨")
        
    else:
        print("\nğŸ”„ ä½¿ç”¨å•ç§Ÿæˆ·åŒæ­¥ç­–ç•¥")
        # å•ç§Ÿæˆ·åœºæ™¯ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
        total_tables = len(db_names) * len(table_names)
        current = 0
        success_count = 0
        
        for db_name in db_names:
            for table_name in table_names:
                current += 1
                print(f"\n[{current}/{total_tables}] å¤„ç†ä¸­...")
                try:
                    sync_single_table_single_tenant(params, db_name, table_name)
                    success_count += 1
                except Exception as e:
                    print(f"âŒ åŒæ­¥å¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()
        
        print(f"\nğŸ‰ å•ç§Ÿæˆ·åŒæ­¥å®Œæˆï¼æˆåŠŸå¤„ç†äº† {success_count}/{total_tables} ä¸ªè¡¨")

if __name__ == "__main__":
    main()